import json, os
import re
from openai import OpenAI
from tqdm import tqdm
from time import sleep
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("âŒ OPENAI_API_KEY not found in .env file. Please check your .env file.")

model_name = "gpt-4o-mini"

client = OpenAI(api_key=api_key)

print(f"âœ“ API key loaded successfully")
print(f"âœ“ Using model: {model_name}")
print(f"âœ“ Generating bilingual QA pairs for OFF-CAMPUS locations (Korean + English)\n")


def load_input_json(json_path):
    """ì§€ì •ëœ ê²½ë¡œì˜ JSON íŒŒì¼ì„ ì½ì–´ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"Loaded {len(data)} off-campus locations from {json_path}")
    return data


def generate_qa_batch(location_info, batch_type, batch_num, language="korean", max_retries=3):
    """
    êµì™¸ ê±´ë¬¼/ì¥ì†Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ìœ í˜•ì˜ QA ìŒì„ ë°°ì¹˜ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    
    batch_type: 'basic', 'route', 'transport', 'complex' ì¤‘ í•˜ë‚˜
    language: 'korean' ë˜ëŠ” 'english'
    """
    location_str = json.dumps(location_info, ensure_ascii=False, indent=2)
    
    # ì–¸ì–´ë³„ ì„¤ì •
    if language == "korean":
        lang_instruction = "í•œêµ­ì–´ë¡œ"
        lang_note = "ë°˜ë§ ìœ„ì£¼, ì¡´ëŒ“ë§ ì¼ë¶€ í˜¼ìš©"
        example_questions = {
            'basic': '"ë¬´í•™ì—¬ê³  ì–´ë”” ìˆì–´?", "ë¬´í•™ì—¬ê³ ëŠ” ë­ì•¼?", "ë¬´í•™ì—¬ê³  ì–´ë–¤ ê³³ì´ì•¼?"',
            'route': '"í•œì–‘ëŒ€ì—­ì—ì„œ ë¬´í•™ì—¬ê³  ì–´ë–»ê²Œ ê°€?", "ë¬´í•™ì—¬ê³ ê¹Œì§€ ê°€ëŠ” ë²• ì•Œë ¤ì¤˜", "ë¬´í•™ì—¬ê³  ê°€ëŠ” ê²½ë¡œ ì„¤ëª…í•´ì¤˜"',
            'transport': '"ë¬´í•™ì—¬ê³ ê¹Œì§€ ê±¸ì–´ì„œ ì–¼ë§ˆë‚˜ ê±¸ë ¤?", "ë¬´í•™ì—¬ê³  ë„ë³´ë¡œ ê°ˆ ìˆ˜ ìˆì–´?", "ë¬´í•™ì—¬ê³  ë²„ìŠ¤ íƒ€ê³  ê°€ì•¼ í•´?"',
            'complex': '"ë¬´í•™ì—¬ê³ ëŠ” ë­ê³  ì–´ë–»ê²Œ ê°€?", "ë¬´í•™ì—¬ê³  ê°€ëŠ”ë° ì‹œê°„ ì–¼ë§ˆë‚˜ ê±¸ë ¤?", "ë¬´í•™ì—¬ê³ ê¹Œì§€ ê°€ì¥ ë¹ ë¥¸ ë°©ë²•ì€?"'
        }
    else:  # english
        lang_instruction = "ì˜ì–´ë¡œ"
        lang_note = "ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ ì˜ì–´ (informal/conversational)"
        example_questions = {
            'basic': '"Where is Muhak Girls\' High School?", "What is Muhak Girls\' High School?", "What kind of place is it?"',
            'route': '"How do I get to Muhak Girls\' High School from Hanyang Univ. Station?", "Can you tell me the way?", "Explain the route please"',
            'transport': '"How long does it take to walk?", "Can I walk there?", "Do I need to take a bus?"',
            'complex': '"What is it and how do I get there?", "How long does it take?", "What\'s the fastest way?"'
        }
    
    # ë°°ì¹˜ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„¤ì • (êµì™¸ ê±´ë¬¼ì— ë§ê²Œ ì¡°ì •)
    prompts = {
        'basic': {
            'count': 8,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 8ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ì¥ì†Œ ì´ë¦„/ìœ„ì¹˜ ì§ˆë¬¸ (3ê°œ)
            - ì¥ì†Œ íŠ¹ì§•/ì¹´í…Œê³ ë¦¬ ì§ˆë¬¸ (3ê°œ)
            - ì¥ì†Œ ì¢…ë¥˜ ì§ˆë¬¸ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['basic']}
            """
        },
        'route': {
            'count': 12,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 12ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - í•œì–‘ëŒ€ì—­/ìº í¼ìŠ¤ì—ì„œ ê°€ëŠ” ë°©ë²• (5ê°œ)
            - êµ¬ì²´ì ì¸ ê²½ë¡œ ì•ˆë‚´ (5ê°œ)
            - ì¤‘ê°„ ì§€ì /ëœë“œë§ˆí¬ ê¸°ì¤€ ê²½ë¡œ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['route']}
            """
        },
        'transport': {
            'count': 12,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 12ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ì†Œìš” ì‹œê°„ ì§ˆë¬¸ (4ê°œ)
            - ì´ë™ ìˆ˜ë‹¨ ì§ˆë¬¸ (4ê°œ)
            - ë„ë³´ ê°€ëŠ¥ ì—¬ë¶€ ì§ˆë¬¸ (2ê°œ)
            - ëŒ€ì¤‘êµí†µ ì´ìš© ì§ˆë¬¸ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['transport']}
            """
        },
        'complex': {
            'count': 8,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 8ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ë³µí•© ì •ë³´ ì§ˆë¬¸ (ì¥ì†Œ íŠ¹ì§• + ê²½ë¡œ) (3ê°œ)
            - ìµœì  ê²½ë¡œ ì§ˆë¬¸ (2ê°œ)
            - ì†Œìš” ì‹œê°„ í¬í•¨ ê²½ë¡œ ì§ˆë¬¸ (2ê°œ)
            - ë¹„êµ ì§ˆë¬¸ (1ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['complex']}
            """
        }
    }
    
    batch_config = prompts[batch_type]
    
    if language == "korean":
        prompt = f"""
ë‹¹ì‹ ì€ í•œì–‘ëŒ€í•™êµ ì£¼ë³€ ì§€ì—­ ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ êµì™¸ ê±´ë¬¼/ì¥ì†Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒë“¤ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸-ë‹µë³€ ìŒì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.

[ì¥ì†Œ ì •ë³´]
```json
{location_str}
```

[ìƒì„± ì§€ì¹¨]
{batch_config['instruction']}

[ì¤‘ìš” ê·œì¹™]
1. ì§ˆë¬¸ì€ êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„± ({lang_note})
2. ì§ˆë¬¸ í‘œí˜„ì„ ìµœëŒ€í•œ ë‹¤ì–‘í•˜ê²Œ:
   - ì¥ì†Œëª… ë³€í˜•: "{location_info.get('name', '')}", ì•½ì¹­ì´ë‚˜ ë³„ì¹­ë„ í™œìš©
   - ì§ˆë¬¸ í˜•ì‹: "~ì–´ë””ì•¼?", "~ì•Œë ¤ì¤˜", "~ì–´ë–»ê²Œ ê°€?", "~ë­ì•¼?", "~ê°ˆ ìˆ˜ ìˆì–´?" ë“±
3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ, ì£¼ì–´ì§„ ì •ë³´ì—ë§Œ ê·¼ê±°
4. ë‹µë³€ ê¸¸ì´: ê²½ë¡œ ì„¤ëª…ì´ ê¸´ ê²½ìš° 200-400ìê¹Œì§€ ê°€ëŠ¥
5. ê° QAëŠ” ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ë‚´ìš©ì´ì–´ì•¼ í•¨ (ì¤‘ë³µ ìµœì†Œí™”)
6. route_descriptionì€ í•œì–‘ëŒ€ì—­ 2ë²ˆ ì¶œêµ¬(ì˜ˆë¬¸)ë¥¼ ì¶œë°œì ìœ¼ë¡œ í•¨
7. estimated_time_walking ì •ë³´ë¥¼ ì ê·¹ í™œìš©
8. transport_type ì •ë³´ (ë„ë³´/ë²„ìŠ¤ ë“±)ë¥¼ ë°˜ì˜
9. êµì™¸ ê±´ë¬¼ì´ë¯€ë¡œ ê±°ë¦¬ê°€ ë©€ë‹¤ëŠ” ì ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:

```json
[
  {{
    "question": "ìƒì„±ëœ ì§ˆë¬¸ 1",
    "answer": "ìƒì„±ëœ ë‹µë³€ 1",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }},
  {{
    "question": "ìƒì„±ëœ ì§ˆë¬¸ 2",
    "answer": "ìƒì„±ëœ ë‹µë³€ 2",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }}
]
```

ì •í™•íˆ {batch_config['count']}ê°œì˜ QA ìŒì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
    else:  # english
        prompt = f"""
You are a Hanyang University area guide expert.
Based on the off-campus location information below, generate natural question-answer pairs in English that students would actually ask.

[Location Information]
```json
{location_str}
```

[Generation Guidelines]
{batch_config['instruction']}

[Important Rules]
1. Questions should be written in natural conversational style ({lang_note})
2. Vary question expressions as much as possible:
   - Location name variations: "{location_info.get('name', '')}", use abbreviations or common names
   - Question formats: "Where is~?", "Can you tell me~?", "How do I get to~?", "What is~?", "Can I walk to~?" etc.
3. Answers should be clear and friendly, based only on the given information
4. Answer length: 200-400 characters for detailed route descriptions
5. Each QA should be clearly distinct (minimize duplication)
6. route_description starts from Exit 2 of Hanyang Univ. Station (Aeji Gate)
7. Actively use estimated_time_walking information
8. Reflect transport_type information (walking/bus etc.)
9. Naturally mention that it's off-campus and relatively far

[Output Format]
Output ONLY in the following JSON array format:

```json
[
  {{
    "question": "Generated question 1",
    "answer": "Generated answer 1",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }},
  {{
    "question": "Generated question 2",
    "answer": "Generated answer 2",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }}
]
```

Generate exactly {batch_config['count']} QA pairs.
"""

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=4000  # 3000 â†’ 4000 (ê¸´ ê²½ë¡œ ì„¤ëª… ëŒ€ì‘)
            )
            content = response.choices[0].message.content.strip()
            
            if not content:
                print(f"  âš ï¸ Empty response for {language} {batch_type} batch {batch_num} (Attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    sleep(2)
                    continue
                return []
            
            # JSON íŒŒì‹±
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', content, re.DOTALL)
            if not json_match:
                json_match = re.search(r'(\[.*?\])', content, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1)
                parsed_list = json.loads(json_str)
                
                if isinstance(parsed_list, list) and len(parsed_list) > 0:
                    print(f"  âœ“ Generated {len(parsed_list)} {language} QAs for {batch_type} batch {batch_num}")
                    return parsed_list
                else:
                    raise ValueError("Parsed JSON is empty or invalid")
            else:
                raise ValueError("No JSON array found in response")
        
        except (json.JSONDecodeError, ValueError) as e:
            print(f"  âš ï¸ Parse error for {language} {batch_type} batch {batch_num}: {e} (Attempt {attempt + 1})")
            if attempt < max_retries - 1:
                sleep(2)
        except Exception as e:
            print(f"  âŒ API error for {language} {batch_type} batch {batch_num}: {e} (Attempt {attempt + 1})")
            if attempt < max_retries - 1:
                sleep(3)
    
    return []


def generate_qa_pairs_for_location(location_info, language="korean"):
    """
    ë‹¨ì¼ êµì™¸ ì¥ì†Œì— ëŒ€í•´ 40ê°œì˜ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë°°ì¹˜ë³„ë¡œ ë‚˜ëˆ ì„œ ìƒì„±í•˜ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
    """
    location_name = location_info.get('name', 'Unknown')
    lang_label = "ğŸ‡°ğŸ‡·" if language == "korean" else "ğŸ‡ºğŸ‡¸"
    print(f"\nğŸ“ {lang_label} Processing: {location_name} ({language.upper()})")
    
    all_qa_pairs = []
    
    # ë°°ì¹˜ë³„ ìƒì„± (ì´ 40ê°œ ëª©í‘œ)
    batches = [
        ('basic', 1),      # 8ê°œ
        ('route', 1),      # 12ê°œ
        ('transport', 1),  # 12ê°œ
        ('complex', 1),    # 8ê°œ
    ]
    
    for batch_type, batch_num in batches:
        qa_batch = generate_qa_batch(location_info, batch_type, batch_num, language)
        all_qa_pairs.extend(qa_batch)
        sleep(1)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
    
    print(f"  ğŸ“Š Total generated: {len(all_qa_pairs)} {language} QA pairs")
    return all_qa_pairs


def generate_all_qa_pairs(input_data):
    """
    ëª¨ë“  êµì™¸ ì¥ì†Œì— ëŒ€í•´ í•œêµ­ì–´ì™€ ì˜ì–´ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    korean_qa_pairs = []
    english_qa_pairs = []
    korean_counter = 0
    english_counter = 0
    
    for location in tqdm(input_data, desc="Generating bilingual QA dataset (Off-campus)"):
        try:
            # í•œêµ­ì–´ QA ìƒì„±
            korean_pairs = generate_qa_pairs_for_location(location, language="korean")
            
            if korean_pairs:
                for qa in korean_pairs:
                    qa_pair = {
                        "id": f"KO_OFF_{korean_counter:05d}",
                        "language": "korean",
                        "question": qa.get("question", ""),
                        "answer": qa.get("answer", ""),
                        "type": qa.get("type", "unknown"),
                        "location_name": qa.get("location_name", ""),
                        "category": location.get("category", ""),
                        "context": location
                    }
                    korean_qa_pairs.append(qa_pair)
                    korean_counter += 1
            else:
                print(f"âš ï¸ Warning: No Korean QA pairs generated for {location.get('name', 'Unknown')}")
            
            sleep(2)  # í•œì˜ ìƒì„± ì‚¬ì´ ëŒ€ê¸°
            
            # ì˜ì–´ QA ìƒì„±
            english_pairs = generate_qa_pairs_for_location(location, language="english")
            
            if english_pairs:
                for qa in english_pairs:
                    qa_pair = {
                        "id": f"EN_OFF_{english_counter:05d}",
                        "language": "english",
                        "question": qa.get("question", ""),
                        "answer": qa.get("answer", ""),
                        "type": qa.get("type", "unknown"),
                        "location_name": qa.get("location_name", ""),
                        "category": location.get("category", ""),
                        "context": location
                    }
                    english_qa_pairs.append(qa_pair)
                    english_counter += 1
            else:
                print(f"âš ï¸ Warning: No English QA pairs generated for {location.get('name', 'Unknown')}")
        
        except Exception as e:
            print(f"âŒ Error processing {location.get('name', 'Unknown')}: {e}")
    
    return korean_qa_pairs, english_qa_pairs


def save_qa_pairs_to_json(qa_pairs, output_path, language):
    """ìƒì„±ëœ QA ìŒ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Saved {language} QA pairs to: {output_path}")


def print_statistics(korean_qa, english_qa):
    """ìƒì„±ëœ QA ë°ì´í„°ì…‹ì˜ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*70)
    print("ğŸ“Š OFF-CAMPUS BILINGUAL DATASET STATISTICS")
    print("="*70)
    
    # í•œêµ­ì–´ í†µê³„
    print("\nğŸ‡°ğŸ‡· KOREAN QA PAIRS")
    print("-" * 70)
    print(f"Total: {len(korean_qa)}")
    
    ko_type_counts = {}
    for qa in korean_qa:
        qa_type = qa.get('type', 'unknown')
        ko_type_counts[qa_type] = ko_type_counts.get(qa_type, 0) + 1
    
    print("\n[By Type]")
    for qa_type, count in sorted(ko_type_counts.items()):
        print(f"  {qa_type:12s}: {count:4d} ({count/len(korean_qa)*100:.1f}%)")
    
    ko_location_counts = {}
    for qa in korean_qa:
        location = qa.get('location_name', 'Unknown')
        ko_location_counts[location] = ko_location_counts.get(location, 0) + 1
    
    print(f"\n[By Location]")
    if len(ko_location_counts) > 0:
        print(f"  Total locations: {len(ko_location_counts)}")
        print(f"  Avg QA per location: {len(korean_qa)/len(ko_location_counts):.1f}")
        print(f"  Min: {min(ko_location_counts.values())}, Max: {max(ko_location_counts.values())}")
    
    # ì˜ì–´ í†µê³„
    print("\n" + "-" * 70)
    print("ğŸ‡ºğŸ‡¸ ENGLISH QA PAIRS")
    print("-" * 70)
    print(f"Total: {len(english_qa)}")
    
    en_type_counts = {}
    for qa in english_qa:
        qa_type = qa.get('type', 'unknown')
        en_type_counts[qa_type] = en_type_counts.get(qa_type, 0) + 1
    
    print("\n[By Type]")
    for qa_type, count in sorted(en_type_counts.items()):
        print(f"  {qa_type:12s}: {count:4d} ({count/len(english_qa)*100:.1f}%)")
    
    en_location_counts = {}
    for qa in english_qa:
        location = qa.get('location_name', 'Unknown')
        en_location_counts[location] = en_location_counts.get(location, 0) + 1
    
    print(f"\n[By Location]")
    if len(en_location_counts) > 0:
        print(f"  Total locations: {len(en_location_counts)}")
        print(f"  Avg QA per location: {len(english_qa)/len(en_location_counts):.1f}")
        print(f"  Min: {min(en_location_counts.values())}, Max: {max(en_location_counts.values())}")
    
    # ì „ì²´ í†µê³„
    print("\n" + "-" * 70)
    print("ğŸŒ COMBINED STATISTICS")
    print("-" * 70)
    print(f"Total QA pairs (Korean + English): {len(korean_qa) + len(english_qa)}")
    print(f"Korean: {len(korean_qa)} ({len(korean_qa)/(len(korean_qa)+len(english_qa))*100:.1f}%)")
    print(f"English: {len(english_qa)} ({len(english_qa)/(len(korean_qa)+len(english_qa))*100:.1f}%)")
    
    print("="*70 + "\n")


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    
    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    input_json_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\hanyang_routes_gpt_1_2km.json"
    korean_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\offcampus_qa_pairs2_korean.json"
    english_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\offcampus_qa_pairs2_english.json"
    combined_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\offcampus_qa_pairs2_combined.json"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(korean_output_path), exist_ok=True)
    
    try:
        # 1. ì…ë ¥ JSON íŒŒì¼ ë¡œë“œ
        input_data = load_input_json(input_json_path)
        
        # 2. í•œì˜ QA ìŒ ìƒì„±
        if isinstance(input_data, list):
            korean_qa, english_qa = generate_all_qa_pairs(input_data)
        elif isinstance(input_data, dict):
            print("Input is a single object, wrapping in a list.")
            korean_qa, english_qa = generate_all_qa_pairs([input_data])
        else:
            print("âŒ Error: Input JSON format is not a list or object.")
            korean_qa, english_qa = [], []
        
        if not korean_qa and not english_qa:
            print("âŒ No QA pairs were generated. Please check your input data and API.")
            exit(1)
        
        # 3. ìƒ˜í”Œ ì¶œë ¥
        print("\n" + "="*70)
        print("ğŸ“ SAMPLE QA PAIRS (OFF-CAMPUS)")
        print("="*70)
        
        if korean_qa:
            print("\nğŸ‡°ğŸ‡· Korean Sample (First 2):")
            for i, qa in enumerate(korean_qa[:2], 1):
                print(f"\n[KO Sample {i}]")
                print(f"Q: {qa['question']}")
                print(f"A: {qa['answer'][:100]}..." if len(qa['answer']) > 100 else f"A: {qa['answer']}")
                print(f"Type: {qa['type']}, Location: {qa['location_name']}")
        
        if english_qa:
            print("\nğŸ‡ºğŸ‡¸ English Sample (First 2):")
            for i, qa in enumerate(english_qa[:2], 1):
                print(f"\n[EN Sample {i}]")
                print(f"Q: {qa['question']}")
                print(f"A: {qa['answer'][:100]}..." if len(qa['answer']) > 100 else f"A: {qa['answer']}")
                print(f"Type: {qa['type']}, Location: {qa['location_name']}")
        
        print("="*70)
        
        # 4. í†µê³„ ì¶œë ¥
        print_statistics(korean_qa, english_qa)
        
        # 5. JSON íŒŒì¼ë¡œ ì €ì¥ (ë¶„ë¦¬)
        save_qa_pairs_to_json(korean_qa, korean_output_path, "Korean")
        save_qa_pairs_to_json(english_qa, english_output_path, "English")
        
        # 6. í†µí•© íŒŒì¼ë„ ì €ì¥ (ì„ íƒì‚¬í•­)
        combined_qa = korean_qa + english_qa
        save_qa_pairs_to_json(combined_qa, combined_output_path, "Combined")
        
        print("\n" + "="*70)
        print("âœ… Successfully generated OFF-CAMPUS bilingual QA dataset!")
        print("="*70)
        print(f"ğŸ‡°ğŸ‡· Korean QA pairs: {len(korean_qa)}")
        print(f"   Target: {len(input_data)} locations Ã— 40 = {len(input_data) * 40}")
        print(f"   Achievement: {len(korean_qa)/(len(input_data)*40)*100:.1f}%")
        
        print(f"\nğŸ‡ºğŸ‡¸ English QA pairs: {len(english_qa)}")
        print(f"   Target: {len(input_data)} locations Ã— 40 = {len(input_data) * 40}")
        print(f"   Achievement: {len(english_qa)/(len(input_data)*40)*100:.1f}%")
        
        print(f"\nğŸŒ Total QA pairs: {len(korean_qa) + len(english_qa)}")
        print("="*70)

    except FileNotFoundError:
        print(f"âŒ Error: Input file not found at {input_json_path}")
    except Exception as e:
        print(f"âŒ An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()
