import torch
import os
import json
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    EarlyStoppingCallback
)
from peft import LoraConfig
from datasets import Dataset
from trl import SFTTrainer
from huggingface_hub import login

# ========================================================================
# 0. HuggingFace ë¡œê·¸ì¸
# ========================================================================
HF_TOKEN = 
try:
    login(token=HF_TOKEN)
    print("âœ… HuggingFace ë¡œê·¸ì¸ ì„±ê³µ\n")
except Exception as e:
    print(f"âš ï¸  ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}\n")

# ========================================================================
# 1. ê²½ë¡œ ë° ëª¨ë¸ ì„¤ì •
# ========================================================================

print("="*70)
print("ğŸ“ í•œì–‘ëŒ€í•™êµ ê¸¸ì•ˆë‚´ AI í•™ìŠµ FINAL (Messages í¬ë§· - ìˆ˜ì •)")
print("="*70)

BASE_DIR = os.getcwd()
DATASET_DIR = os.path.join(BASE_DIR, "dataset_final")

QA_TRAIN_FILES = [
    os.path.join(DATASET_DIR, "train_data_1km_messages.json"),
    os.path.join(DATASET_DIR, "train_data_2km_messages.json"),
    os.path.join(DATASET_DIR, "train_data_in_messages.json")
]

QA_VAL_FILES = [
    os.path.join(DATASET_DIR, "val_data_1km_messages.json"),
    os.path.join(DATASET_DIR, "val_data_2km_messages.json"),
    os.path.join(DATASET_DIR, "val_data_in_messages.json")
]

MODEL_ID = "nlpai-lab/ko-gemma-2b-v1"
OUTPUT_DIR = "./output/gemma-2b-hanyang-guide-final"
ADAPTER_PATH = "./output/gemma-2b-hanyang-guide-lora-final"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(ADAPTER_PATH, exist_ok=True)

print(f"ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸: {MODEL_ID}")
print(f"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}")
print("="*70 + "\n")

# ========================================================================
# 2. GPU í™•ì¸
# ========================================================================
print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸")
if torch.cuda.is_available():
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
else:
    print("âš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
print()

# ========================================================================
# 3. QLoRA ì„¤ì •
# ========================================================================

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)

print("="*70)
print("ğŸ“‹ í•™ìŠµ ì„¤ì •")
print("="*70)
print(f"ëª¨ë¸ í¬ê¸°: 2B parameters")
print(f"LoRA rank: 16")
print(f"LoRA alpha: 32")
print("="*70 + "\n")

# ========================================================================
# 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ========================================================================
print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘... ({MODEL_ID})")

try:
    # â­ ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì • (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì‹œ)
    # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ìºì‹œê°€ ìˆë‹¤ë©´ ì‚¬ìš©
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        local_files_only=False,  # Trueë¡œ ë³€ê²½í•˜ë©´ ì™„ì „ ì˜¤í”„ë¼ì¸
        resume_download=True,    # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
    )
    tokenizer.padding_side = 'right'
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        local_files_only=False,  # Trueë¡œ ë³€ê²½í•˜ë©´ ì™„ì „ ì˜¤í”„ë¼ì¸
        resume_download=True,    # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
    )
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"ğŸ“ Chat template ì¡´ì¬: {tokenizer.chat_template is not None}")
    print(f"ğŸ”¢ Vocab size: {tokenizer.vocab_size:,}")
    print()

except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# ========================================================================
# 5. ë°ì´í„°ì…‹ ë¡œë“œ
# ========================================================================
print("="*70)
print("ğŸ“‚ ë°ì´í„° ë¡œë“œ (messages í¬ë§·)")
print("="*70)

def load_messages_data(file_paths, dataset_type="Train"):
    """messages í¬ë§· ë°ì´í„° ë¡œë“œ í›„ ì¦‰ì‹œ chat template ì ìš©"""
    all_texts = []
    
    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"âš ï¸  {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                for item in data:
                    if "messages" in item and isinstance(item["messages"], list):
                        # â­ ì¦‰ì‹œ chat template ì ìš©
                        try:
                            text = tokenizer.apply_chat_template(
                                item["messages"],
                                tokenize=False,
                                add_generation_prompt=False
                            )
                            all_texts.append(text)
                        except Exception as e:
                            print(f"âš ï¸  Chat template ì ìš© ì‹¤íŒ¨: {e}")
                            print(f"   Messages: {item['messages']}")
                    else:
                        print(f"âš ï¸  ì˜ëª»ëœ í¬ë§·: {item}")
                
                print(f"âœ… {os.path.basename(file_path)}: {len(data)}ê°œ ë¡œë“œ")
            else:
                print(f"âš ï¸  {file_path}ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ“Š ì´ {dataset_type} ë°ì´í„°: {len(all_texts)}ê°œ")
    return all_texts

# Train ë°ì´í„° ë¡œë“œ (ì´ë¯¸ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸)
print("\n[Train ë°ì´í„°]")
train_texts = load_messages_data(QA_TRAIN_FILES, "Train")

# Validation ë°ì´í„° ë¡œë“œ (ì´ë¯¸ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸)
print("\n[Validation ë°ì´í„°]")
val_texts = load_messages_data(QA_VAL_FILES, "Validation")

if not val_texts:
    print("âš ï¸  Validation ë°ì´í„°ê°€ ì—†ì–´ Trainì—ì„œ 10% ë¶„ë¦¬")
    split_idx = int(len(train_texts) * 0.9)
    val_texts = train_texts[split_idx:]
    train_texts = train_texts[:split_idx]

# Dataset ë³€í™˜ (text í•„ë“œ ì‚¬ìš©)
train_dataset = Dataset.from_dict({"text": train_texts})
eval_dataset = Dataset.from_dict({"text": val_texts})

print("\n" + "="*70)
print("ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹")
print("="*70)
print(f"Train: {len(train_dataset):,}ê°œ")
print(f"Eval:  {len(eval_dataset):,}ê°œ")
print(f"Total: {len(train_dataset) + len(eval_dataset):,}ê°œ")
print("="*70 + "\n")

# ìƒ˜í”Œ í™•ì¸
if len(train_dataset) > 0:
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„°:")
    print("-" * 70)
    sample_text = train_dataset[0]['text']
    print(f"í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì):")
    print(sample_text[:500])
    print("...")
    print("-" * 70 + "\n")

# ========================================================================
# 6. â­ formatting_func ë¶ˆí•„ìš” (ì´ë¯¸ í¬ë§·íŒ…ë¨)
# ========================================================================
# ë°ì´í„°ê°€ ì´ë¯¸ chat templateì´ ì ìš©ëœ textì´ë¯€ë¡œ formatting_func ë¶ˆí•„ìš”!

# ========================================================================
# 7. SFTTrainer ì„¤ì •
# ========================================================================
print("âš™ï¸  Trainer ì„¤ì • ì¤‘...\n")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    
    # ì—í­ ì„¤ì •
    num_train_epochs=3,
    
    # ë°°ì¹˜ í¬ê¸°
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    gradient_checkpointing=True,
    max_grad_norm=1.0,
    
    # ì˜µí‹°ë§ˆì´ì €
    optim="paged_adamw_8bit",
    
    # í•™ìŠµë¥ 
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.01,
    
    # í‰ê°€ ì „ëµ
    eval_strategy="steps",
    eval_steps=100,
    save_steps=100,
    save_total_limit=3,
    
    # ì •ë°€ë„
    fp16=True,
    
    # ìµœê³  ëª¨ë¸ ì„ íƒ
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    
    # ë¡œê¹…
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=10,
    report_to="tensorboard",
)

print("="*70)
print("ğŸ“‹ ìµœì¢… í•™ìŠµ ì„¤ì •")
print("="*70)
print(f"ì‹¤ì§ˆ ë°°ì¹˜: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
total_steps = len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)
print(f"ì˜ˆìƒ ìŠ¤í…: {total_steps:,}")
print(f"í•™ìŠµë¥ : {training_args.learning_rate}")
print("="*70 + "\n")

# ì¡°ê¸° ì¢…ë£Œ
early_stopping = EarlyStoppingCallback(early_stopping_patience=3)

# â­ SFTTrainer (formatting_funcìœ¼ë¡œ text ë°˜í™˜)
def formatting_func(example):
    """ì´ë¯¸ í¬ë§·íŒ…ëœ textë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    return example["text"]

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    formatting_func=formatting_func,  # â­ textë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    callbacks=[early_stopping],
)

# ========================================================================
# 8. í›ˆë ¨ ì‹œì‘
# ========================================================================
print("="*70)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("="*70)
print("ğŸ’¡ ì ìš©ëœ ë°©ì‹:")
print("   âœ… ë°ì´í„° ë¡œë“œ ì‹œ ì¦‰ì‹œ chat template ì ìš©")
print("   âœ… dataset_text_field='text' ì‚¬ìš©")
print("   âœ… formatting_func ë¶ˆí•„ìš” (ì´ë¯¸ í¬ë§·íŒ…ë¨)")
print("="*70 + "\n")

try:
    trainer.train()
    
    print("\n" + "="*70)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("="*70)

except KeyboardInterrupt:
    print("\nâš ï¸  í•™ìŠµ ì¤‘ë‹¨ë¨.")
except Exception as e:
    print(f"\nâŒ í•™ìŠµ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()

# ========================================================================
# 9. LoRA ì–´ëŒ‘í„° ì €ì¥
# ========================================================================
print("\n" + "="*70)
print("ğŸ’¾ LoRA ì–´ëŒ‘í„° ì €ì¥")
print("="*70)

try:
    trainer.model.save_pretrained(ADAPTER_PATH)
    tokenizer.save_pretrained(ADAPTER_PATH)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {ADAPTER_PATH}")
    
    print("\n" + "="*70)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print("="*70)
    print(f"ğŸ“ LoRA ì–´ëŒ‘í„°: {ADAPTER_PATH}")
    print("\nâš ï¸  ì¤‘ìš”: ì¶”ë¡  ì‹œì—ë„ tokenizer.apply_chat_template() ì‚¬ìš©!")
    print("="*70)
    
except Exception as e:
    print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

print("\nâœ… ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ")
print("="*70)
