import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from huggingface_hub import login

# ------------------------------------------------------------------------
# 0. HuggingFace ë¡œê·¸ì¸
# ------------------------------------------------------------------------
HF_TOKEN = 
try:
    login(token=HF_TOKEN)
    print("âœ… HuggingFace ë¡œê·¸ì¸ ì„±ê³µ\n")
except Exception as e:
    print(f"âš ï¸  HuggingFace ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}\n")

# ------------------------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì •
# ------------------------------------------------------------------------
BASE_MODEL = "nlpai-lab/ko-gemma-2b-v1"
ADAPTER_PATH = r"C:\Users\jaeyu\Desktop\gemma\LLM\output\gemma-2b-hanyang-guide-lora-final"
MERGED_PATH = r"C:\Users\jaeyu\Desktop\gemma\LLM\output\gemma-2b-hanyang-final-merged"

# ì–´ëŒ‘í„° ê²½ë¡œ í™•ì¸
if not os.path.exists(ADAPTER_PATH):
    print(f"âŒ ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ADAPTER_PATH}")
    print("ğŸ’¡ ë¨¼ì € local_training_script.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
    exit(1)

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(MERGED_PATH, exist_ok=True)

# ------------------------------------------------------------------------
# 2. ëª¨ë¸ ë³‘í•© í”„ë¡œì„¸ìŠ¤
# ------------------------------------------------------------------------
print("=" * 70)
print("ğŸ”„ ëª¨ë¸ ë³‘í•© ì‹œì‘")
print("=" * 70)
print(f"ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸: {BASE_MODEL}")
print(f"ğŸ”— LoRA ì–´ëŒ‘í„°: {ADAPTER_PATH}")
print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {MERGED_PATH}")
print("=" * 70 + "\n")

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸ’¾ ì´ˆê¸° GPU ë©”ëª¨ë¦¬:")
    print(f"   í• ë‹¹: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"   ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\n")

# ------------------------------------------------------------------------
# ë°©ë²• 1: ìˆ˜ë™ ë³‘í•© (ê°€ì¥ ì•ˆì •ì ) â­ ì¶”ì²œ
# ------------------------------------------------------------------------

print("1ë‹¨ê³„: ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ...")
try:
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map='cpu',  # CPUì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë³‘í•©
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
except Exception as e:
    print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

print("2ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë“œ...")
try:
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)
    
    # pad token í™•ì¸ ë° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        base_model.resize_token_embeddings(len(tokenizer))
        print("   âš ï¸  pad_token ì¶”ê°€ë¨")
    
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n")
except Exception as e:
    print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
    # ë² ì´ìŠ¤ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    print("   â†’ ë² ì´ìŠ¤ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        base_model.resize_token_embeddings(len(tokenizer))

print("3ë‹¨ê³„: LoRA ì–´ëŒ‘í„° ë¡œë“œ...")
try:
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_PATH,
        device_map='cpu'
    )
    print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ\n")
except Exception as e:
    print(f"âŒ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ğŸ’¡ ì–´ëŒ‘í„° íŒŒì¼ ê²½ë¡œì™€ í¬ë§·ì„ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

print("4ë‹¨ê³„: ì–´ëŒ‘í„° ë³‘í•© ì‹œë„...")
merged_model = None

# ë°©ë²• 4-A: merge_and_unload() ì‹œë„
try:
    merged_model = model.merge_and_unload()
    print("âœ… merge_and_unload() ì„±ê³µ!\n")
except Exception as e:
    print(f"âš ï¸  merge_and_unload() ì‹¤íŒ¨: {e}")
    print("   â†’ ëŒ€ì•ˆ: ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë°©ì‹ ì‚¬ìš©\n")
    
    # ë°©ë²• 4-B: ìˆ˜ë™ìœ¼ë¡œ ë³‘í•©ëœ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    try:
        model.eval()
        with torch.no_grad():
            # PEFT ëª¨ë¸ì˜ base_model ì†ì„±ì—ì„œ ë³‘í•©ëœ ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°
            if hasattr(model, 'base_model'):
                if hasattr(model.base_model, 'model'):
                    merged_model = model.base_model.model
                else:
                    merged_model = model.base_model
            else:
                merged_model = model.model
        
        print("âœ… ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì™„ë£Œ!\n")
    except Exception as e:
        print(f"âŒ ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì¶”ì¶œë„ ì‹¤íŒ¨: {e}")
        exit(1)

if merged_model is None:
    print("âŒ ëª¨ë¸ ë³‘í•©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    exit(1)

print("5ë‹¨ê³„: PEFT ê´€ë ¨ ì†ì„± ì •ë¦¬...")
# PEFT ê´€ë ¨ ì†ì„± ëª¨ë‘ ì œê±°
attrs_to_remove = [
    'peft_config', 
    'active_adapter', 
    'active_adapters', 
    '_hf_peft_config_loaded',
    'peft_type',
    'base_model_prefix'
]

for attr in attrs_to_remove:
    try:
        if hasattr(merged_model, attr):
            delattr(merged_model, attr)
            print(f"   âœ“ {attr} ì œê±°ë¨")
    except (AttributeError, TypeError):
        # ì†ì„±ì´ í”„ë¡œí¼í‹°ë‚˜ íŠ¹ìˆ˜ ì†ì„±ì¸ ê²½ìš° ë¬´ì‹œ
        pass

print("âœ… ì†ì„± ì •ë¦¬ ì™„ë£Œ\n")

print("6ë‹¨ê³„: ë³‘í•©ëœ ëª¨ë¸ ì €ì¥...")
try:
    # ë¨¼ì € safe_serializationìœ¼ë¡œ ì €ì¥ ì‹œë„
    merged_model.save_pretrained(
        MERGED_PATH,
        safe_serialization=True,
        max_shard_size="2GB"
    )
    tokenizer.save_pretrained(MERGED_PATH)
    print(f"âœ… ëª¨ë¸ì´ {MERGED_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
    
except Exception as e:
    print(f"âš ï¸  safe_serialization ì €ì¥ ì‹¤íŒ¨: {e}")
    print("   â†’ ëŒ€ì•ˆ: PyTorch ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì €ì¥ ì‹œë„...\n")
    
    try:
        # PyTorch ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì €ì¥
        merged_model.save_pretrained(
            MERGED_PATH,
            safe_serialization=False,
            max_shard_size="2GB"
        )
        tokenizer.save_pretrained(MERGED_PATH)
        print(f"âœ… PyTorch ë°©ì‹ìœ¼ë¡œ ì €ì¥ ì™„ë£Œ!\n")
    except Exception as e2:
        print(f"âš ï¸  save_pretrainedë„ ì‹¤íŒ¨: {e2}")
        print("   â†’ ìµœí›„ì˜ ëŒ€ì•ˆ: state_dict ë°©ì‹ìœ¼ë¡œ ì €ì¥...\n")
        
        try:
            # state_dict ì§ì ‘ ì €ì¥
            torch.save(
                merged_model.state_dict(), 
                os.path.join(MERGED_PATH, "pytorch_model.bin")
            )
            tokenizer.save_pretrained(MERGED_PATH)
            # config íŒŒì¼ë„ ì €ì¥
            merged_model.config.save_pretrained(MERGED_PATH)
            print(f"âœ… state_dict ë°©ì‹ìœ¼ë¡œ ì €ì¥ ì™„ë£Œ!\n")
        except Exception as e3:
            print(f"âŒ ëª¨ë“  ì €ì¥ ë°©ë²• ì‹¤íŒ¨: {e3}")
            exit(1)

print("=" * 70)
print("âœ… ëª¨ë¸ ë³‘í•© ì™„ë£Œ!")
print("=" * 70)

# GPU ë©”ëª¨ë¦¬ ìƒíƒœ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
if torch.cuda.is_available():
    print(f"\nğŸ’¾ ìµœì¢… GPU ë©”ëª¨ë¦¬:")
    print(f"   í• ë‹¹: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"   ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\n")

# ------------------------------------------------------------------------
# 7. ê²€ì¦: ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
# ------------------------------------------------------------------------
print("=" * 70)
print("ğŸ§ª ì €ì¥ëœ ëª¨ë¸ ê²€ì¦ ì¤‘...")
print("=" * 70)

try:
    test_model = AutoModelForCausalLM.from_pretrained(
        MERGED_PATH,
        device_map='cpu',
        torch_dtype=torch.float32
    )
    test_tokenizer = AutoTokenizer.from_pretrained(MERGED_PATH)
    print("âœ… ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ! ë³‘í•©ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")
    
    # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
    test_questions = [
        "ì—­ì‚¬ê´€ ì–´ë”” ìˆì–´?",
        "Where is the History Hall?"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}]")
        print(f"Q: {question}")
        
        prompt = f"<bos><start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n"
        inputs = test_tokenizer(prompt, return_tensors="pt")
        
        print(f"   âœ“ í† í¬ë‚˜ì´ì € ì‘ë™ (ì…ë ¥ í† í° ìˆ˜: {inputs['input_ids'].shape[1]})")
        
        # ì§§ì€ ìƒì„± í…ŒìŠ¤íŠ¸
        try:
            with torch.no_grad():
                outputs = test_model.generate(
                    **inputs,
                    max_new_tokens=50,
                    temperature=0.7,
                    do_sample=True
                )
            
            response = test_tokenizer.decode(outputs[0], skip_special_tokens=False)
            if "<start_of_turn>model" in response:
                answer = response.split("<start_of_turn>model")[1].split("<end_of_turn>")[0].strip()
                print(f"A: {answer[:100]}..." if len(answer) > 100 else f"A: {answer}")
            else:
                print(f"A: {response[:100]}...")
            
            print("   âœ“ ì¶”ë¡  í…ŒìŠ¤íŠ¸ í†µê³¼")
            
        except Exception as e:
            print(f"   âš ï¸  ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print("   (ëª¨ë¸ì€ ì €ì¥ë˜ì—ˆì§€ë§Œ ì¶”ë¡ ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
except Exception as e:
    print(f"âš ï¸  ê²€ì¦ ì‹¤íŒ¨: {e}")
    print("   ëª¨ë¸ì€ ì €ì¥ë˜ì—ˆì§€ë§Œ ë¡œë“œì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

print("\n" + "=" * 70)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("=" * 70)
print(f"ğŸ“ ìµœì¢… ì €ì¥ ê²½ë¡œ: {MERGED_PATH}")
print("\nğŸ’¡ ëª¨ë¸ ì‚¬ìš© ë°©ë²•:")
print("   from transformers import AutoModelForCausalLM, AutoTokenizer")
print(f"   model = AutoModelForCausalLM.from_pretrained('{MERGED_PATH}')")
print(f"   tokenizer = AutoTokenizer.from_pretrained('{MERGED_PATH}')")
print("=" * 70)
