# QLoRAë¥¼ ì´ìš©í•œ Gemma-2Bì˜ í•œì–‘ëŒ€í•™êµ ê¸¸ì•ˆë‚´ íŠ¹í™” íŒŒì¸íŠœë‹ 
AI+X ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸

# Members
- ê³ ì¬ìœ¤, ìœµí•©ì „ìê³µí•™ë¶€, jaeyun2448@naver.com
- ê¶Œì„±ê·¼, ì›ìë ¥ê³µí•™ê³¼, gbdlzlemr02@gmail.com
- ì‹ ì¤€í¬, ê¸°ê³„ê³µí•™ë¶€, shinjh0331@naver.com
- í•œì¸ê¶Œ, ê¸°ê³„ê³µí•™ë¶€, humanaeiura1023@gmail.com
  
# Index
1. Proposal
2. Base-model
3. Datasets
4. Methodology
5. Evaluation & Analysis
6. direction for improvement
7. Model use(additional progress)
  
# Proposal
- ë™ê¸° ë° ëª©í‘œ
  
&nbsp; ë‹¤ë“¤ ìƒˆë‚´ê¸° ë•Œì— ê°€ê³ ì í•˜ëŠ” ê±´ë¬¼ê¹Œì§€ì˜ ê²½ë¡œë¥¼ ì˜ ì•Œì§€ ëª»í•´ ë‹¹í™©í–ˆë˜ ê²½ìš°ê°€ ìˆì—ˆì„ ê²ƒì…ë‹ˆë‹¤. ì €í¬ëŠ” êµë‚´ ê±´ë¬¼ê³¼ í•œì–‘ëŒ€í•™êµ ì£¼ë³€ ê±´ë¬¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•ˆë‚´í•´ì£¼ëŠ” ì±—ë´‡ì„ ë§Œë“œëŠ”ë° ëª©ì ì„ ë‘ê³  í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. êµë‚´ ê±´ë¬¼ë“¤ì˜ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²½ë¡œ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê³  ì´ë¥¼ Ko-gemmaëª¨ë¸ì„ base-modelë¡œ í•˜ì—¬ íŒŒì¸íŠœë‹í•¨ìœ¼ë¡œì„œ í•œì–‘ëŒ€ ê¸¸ì•ˆë‚´ì— íŠ¹í™”ëœ SLM(Small Language Model)ì„ êµ¬ì„±í•˜ëŠ”ë° ì´ˆì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.

- ì§„í–‰ ê³¼ì • ê°œìš”

1. íŒŒì¸íŠœë‹í•  Base-modelì„ ì„ ì •í•˜ê³  ëª¨ë¸ í† í¬ë‚˜ì´ì €ì— ë§ëŠ” ìì²´ì ì¸ í•œì–‘ëŒ€ ì£¼ë³€ ê±´ë¬¼ ê¸¸ì•ˆë‚´ ë°ì´í„°ì…‹ êµ¬ì¶•
2. ìƒì„±í•œ ë°ì´í„°ì…‹ì„ í†µí•´ ëª¨ë¸ íŒŒì¸íŠœë‹ ë° ì „ì´í•™ìŠµ ì§„í–‰
3. ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° ì¶”ë¡  ê²°ê³¼
4. ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê°œì„  ë°©í–¥ ì œì‹œ
5. ëª¨ë¸ í™œìš© í”„ë¡œì íŠ¸(additional progress)


# Base-model
&nbsp; ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì´ Local PC(RTX 3060ti 8GB VRAM)ê³¼ Google Colab(T4 GPU 15GB)ë¡œ ë©”ëª¨ë¦¬ê°€ í•œì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— í° LLMëª¨ë¸ì„ í•™ìŠµí•˜ê¸°ì—ëŠ” ë¬´ë¦¬ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, í•™ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ì„œë„ ì„±ëŠ¥ì´ ì¤€ìˆ˜í•œ ëª¨ë¸ì„ ì„ ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì•„ë˜ NVIDIDAì—ì„œ ì œì‹œí•œ SLM(Small Language Model)ëª¨ë¸ ë³„ ì´ˆë‹¹ í† í° ìˆ˜ë¥¼ ë¹„êµí•œ í‘œë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.

<img width="896" height="484" alt="image" src="https://github.com/user-attachments/assets/e627db24-fff9-4739-8bd6-cfeae036fe64" />

[&nbsp;](https://www.jetson-ai-lab.com/tutorial_slm.html) 

&nbsp; ì €í¬ í”„ë¡œì íŠ¸ëŠ” í•œì–‘ëŒ€í•™êµì˜ ì •ë³´, íŠ¹íˆ ê¸¸ì•ˆë‚´ ì •ë³´ì— ëŒ€í•´ì„œ ì•ˆë‚´í•˜ëŠ” ì¢…í•© ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ì´ ëª¨ë¸ì„ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì¸í„°ë„·ì´ë‚˜ í´ë¼ìš°ë“œ ì‹œìŠ¤í…œì´ ì•„ë‹Œ, Local ì„ë² ë””ë“œ ì‹œìŠ¤í…œì— íƒ‘ì¬í•˜ì—¬ ì§€ì •ëœ ì¥ì†Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸¸ì„ ì•ˆë‚´í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ë ¤ê³  í•˜ì˜€ìŠµë‹ˆë‹¤. ìœ„ì˜ í‘œëŠ” ì„ë² ë””ë“œ ì‹œìŠ¤í…œì¸ NVIDIA Jetson orin nano / AGX orin ì—ì„œ SLMëª¨ë¸ì„ ì‘ë™ì‹œí‚¤ê³  ì¸¡ì •í•œ ë°ì´í„°ì´ê¸°ì— í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì í•©í•˜ë‹¤ê³  ìƒê°í•˜ì˜€ìŠµë‹ˆë‹¤. ìœ„ì˜ í‘œë¥¼ ë³´ì•˜ì„ ë•Œ, Googleì˜ Gemmaëª¨ë¸ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ 2Bì´ê³  ì´ˆë‹¹ í† í° ìƒì„± ê°œìˆ˜ê°€ 27/75ê°œë¡œ í¬ê¸°ì™€ ì„±ëŠ¥ì— ë¹„í•´ ë¹ ë¥´ê²Œ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&nbsp; ì¶”ê°€ì ìœ¼ë¡œ Gemma-2Bëª¨ë¸ì€ ë‹¤êµ­ì–´ ëª¨ë¸ì´ì§€ë§Œ, í•œêµ­ì–´ ë°ì´í„°ì— ëŒ€í•œ í•™ìŠµì´ ë¶€ì¡±í•˜ì—¬ í•œì–‘ëŒ€í•™êµ ê¸¸ì•ˆë‚´ë¼ëŠ” íŠ¹ì •í•œ ì •ë³´ì— ëŒ€í•œ í•™ìŠµì´ ë¶€ì •í™•í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, Gemma-2B-itëª¨ë¸ì„ í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•œ ê³ ë ¤ëŒ€í•™êµì˜ Ko-gemma(gemma-ko-2B-v1)ëª¨ë¸ì„ ìµœì¢… base-modelë¡œ ì„ ì •í•˜ì—¬ í•œì–‘ëŒ€í•™êµ ê¸¸ì•ˆë‚´ë¼ëŠ” íŠ¹ì •í•œ í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

<img width="675" height="279" alt="image" src="https://github.com/user-attachments/assets/2e1480e6-0f2a-494c-b525-90509924d0d5" />

https://github.com/KU-HIAI/Ko-Gemma


# Datasets
&nbsp; í•œì–‘ëŒ€í•™êµ ì£¼ë³€ ê±´ë¬¼ë“¤ì— ëŒ€í•œ ê¸¸ì•ˆë‚´ ë°ì´í„° ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìì²´ì ìœ¼ë¡œ ì œì‘í•œ ë‹¤ìŒ í•™ìŠµì„ ì§„í–‰í•˜ê¸°ë¡œ ê²°ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ë³´ë‹¤ ì •í™•í•œ ë°ì´í„°ì…‹ì„ ì œì‘í•˜ê¸° ìœ„í•´ Naver APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê±´ë¬¼ ë° ê¸¸ì•ˆë‚´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì´ë¥¼ OPENAI APIë¥¼ í™œìš©í•´ ì§ˆë¬¸-ëŒ€ë‹µ ìŒì˜ QAë°ì´í„°ì…‹ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. Naver APIì˜ ë„¤ì´ë²„ ì§€ë„, ë„¤ì´ë²„ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í™œìš©í•´ í•œì–‘ëŒ€ ë‚´ë¶€ ê±´ë¬¼ë“¤ê³¼ í•œì–‘ëŒ€í•™êµ 2ë²ˆì¶œêµ¬ì¸ ì˜ˆì§€ë¬¸ì„ ê¸°ì¤€ìœ¼ë¡œ ë°˜ê²½ 2kmë‚´ì˜ ì£¼ìš” ì‹œì„¤ë“¤ì˜ ì´ë¦„ê³¼ ì˜ˆì§€ë¬¸ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬, ê¸¸ì•ˆë‚´ ê²½ë¡œ, ì†Œìš” ì‹œê°„ ë“±ì˜ ì •ë³´ë¥¼ ì œê³µë°›ì•„ jsoníŒŒì¼ë¡œ 1ì°¨ì ì¸ ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµë‚´ ê±´ë¬¼ 71ê°œ, ì—ì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1kmì˜ êµì™¸ ì£¼ìš” ê±´ë¬¼ 36ê°œ, ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1-2kmì˜ êµì™¸ ì£¼ìš” ê±´ë¬¼ 62ê°œë¡œ êµ¬ì„±í•˜ì˜€ìœ¼ë©°, ê±´ë¬¼ì˜ ìœ„ì¹˜ íŠ¹ì„±ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ë‹´ì•˜ìŠµë‹ˆë‹¤. ê³µí†µì ìœ¼ë¡œëŠ” ê±´ë¬¼ ì´ë¦„, ì†Œìš” ì‹œê°„, ê²½ë¡œ ì•ˆë‚´ ë“±ì˜ ë°ì´í„°ë¥¼ ë‹´ê³  ìˆì§€ë§Œ, êµë‚´ ê±´ë¬¼ì˜ ê²½ìš° ê±´ë¬¼ê³¼ ì¸ì ‘í•œ ë‹¤ë¥¸ ê±´ë¬¼ë“¤ê³¼ì˜ ìœ„ì¹˜ ê´€ê³„ì™€ ê±´ë¬¼ ë²ˆí˜¸ë¼ëŠ” ë°ì´í„°ë¥¼, êµì™¸ ê±´ë¬¼ì€ ê±´ë¬¼ì˜ ì¹´í…Œê³ ë¦¬ì™€ 1-2kmì— ìœ„ì¹˜í•œ ê±´ë¬¼ì€ ëŒ€ì¤‘êµí†µì„ ì´ìš©í•œ ê²½ë¡œ ë°ì´í„°ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

<img width="474" height="150" alt="image" src="https://github.com/user-attachments/assets/dfe7f487-46cb-440d-b01d-b03066daf85e" />


&nbsp; Naver APIë¥¼ ì´ìš©í•´ ìƒì„±í•œ ì •ë³´ ë°ì´í„°ë¥¼ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ QAë°ì´í„°ë¡œ ì¬êµ¬ì„±í•˜ê¸° ìœ„í•´ OPENAI APIë¥¼ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. OPENAI APIì˜ GPT-4o-miniì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ í•œ ê±´ë¬¼ë‹¹ 40ê°œì˜ ì§ˆë¬¸-ëŒ€ë‹µ ìŒì„ ìƒì„±í•˜ì˜€ìœ¼ë©°, íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ 40ê°œì˜ ì§ˆë¬¸-ëŒ€ë‹µì„ Basic(ê±´ë¬¼ ê¸°ë³¸ ì •ë³´), Route(ê²½ë¡œ ì•ˆë‚´), Location(ê±´ë¬¼ ìœ„ì¹˜, ì£¼ë³€ ê±´ë¬¼), Complex(ë³µí•© ì§ˆë¬¸) 4ê°œì˜ typeìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •í•´ì§„ ì§ˆë¬¸ì´ ì•„ë‹Œ ë‹¤ì–‘í•œ ë³µí•© ì§ˆë¬¸ì—ë„ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì˜€ìœ¼ë©°, ëª¨ë¸ì˜ overfittingì„ ë°©ì§€í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì˜ì–´ ë°ì´í„°ì…‹ì€ í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ OPENAI APIë¡œ ë²ˆì—­í•˜ì—¬ ì œì‘í•˜ì˜€ìœ¼ë©°, Ko-gemmaëª¨ë¸ì˜ tokenizer_config.jsonì„ í™•ì¸í•´ë³´ë©´, ëª¨ë¸ì˜ í•™ìŠµê³¼ ì…ë ¥ ë°ì´í„°ë¥¼ messageêµ¬ì¡°ë¡œ ë°›ëŠ”ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì—¬ ì´ì— ë§ì¶”ì–´ ë°ì´í„°ì…‹ì„ messgaeêµ¬ì¡°ë¡œ ì¬êµ¬ì„±í•˜ì˜€ìœ¼ë©°, íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ì˜ ìˆœì„œë¥¼ ë¬´ì‘ìœ„ì ìœ¼ë¡œ ë°°ì¹˜í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì„ ê±°ì³ ì´ 13343ê°œì˜ QAë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì˜€ìœ¼ë©° ëª¨ë¸í•™ìŠµì— ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

<img width="475" height="201" alt="image" src="https://github.com/user-attachments/assets/aca21fb8-d8c4-4026-aa4b-63e2df90dd75" /><img width="309" height="232" alt="image" src="https://github.com/user-attachments/assets/82078196-e280-47b2-bb48-8e6ae61fd689" />

OPENAI APIì‚¬ìš© jsonë°ì´í„°ì…‹ ìƒì„± ì½”ë“œ ê°„ë‹¨ ì„¤ëª…

gpt,py (êµë‚´ ê±´ë¬¼ì— ëŒ€í•œ QAë°ì´í„°ì…‹ ìƒì„±)

### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œí•˜ê³  OPENAI APIì—ì„œ ë°œê¸‰ë°›ì€ í‚¤ë¥¼ í†µí•´ gpt-4o-miniëª¨ë¸ ë¡œë“œ
```python
# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")

model_name = "gpt-4o-mini"

client = OpenAI(api_key=api_key)

print(f"âœ“ API key loaded successfully")
print(f"âœ“ Using model: {model_name}")
print(f"âœ“ Generating bilingual QA pairs (Korean + English)\n")
```

### 2. NAVER APIë¡œ ìƒì„±í•œ ê¸°ì´ˆ ë°ì´í„° json ë¡œë“œ
```python
def load_input_json(json_path):
    """ì§€ì •ëœ ê²½ë¡œì˜ JSON íŒŒì¼ì„ ì½ì–´ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"Loaded {len(data)} buildings from {json_path}")
    return data
```

### 3. ê±´ë¬¼ ì •ë³´ë‹¹ ìƒì„±í•  ì •ë³´ typeêµ¬ë¶„
&nbsp; QAë°ì´í„° typeì€ basic 10ê°œ, route 12ê°œ, location10ê°œ, complex8ê°œë¡œ êµ¬ì„±í•˜ì˜€ìœ¼ë©°, í”„ë¡¬í”„íŠ¸ë¥¼ ì œì‘í•˜ì—¬ ê° typeë³„ êµ¬ì²´ì ì¸ ë°ì´í„° êµ¬ì„± ë°©ì‹ì„ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
def generate_qa_batch(building_info, batch_type, batch_num, language="korean", max_retries=3):

    building_str = json.dumps(building_info, ensure_ascii=False, indent=2)
    
    # ì–¸ì–´ë³„ ì„¤ì •
    if language == "korean":
        lang_instruction = "í•œêµ­ì–´ë¡œ"
        lang_note = "ë°˜ë§ ìœ„ì£¼, ì¡´ëŒ“ë§ ì¼ë¶€ í˜¼ìš©"
        example_questions = {
            'basic': '"ì—­ì‚¬ê´€ ì–´ë”” ìˆì–´?", "101ê´€ì€ ë­ì•¼?", "ì—­ì‚¬ê´€ì€ ëª‡ ì¸µì´ì•¼?"',
            'route': '"ì˜ˆì§€ë¬¸ì—ì„œ ì—­ì‚¬ê´€ ì–´ë–»ê²Œ ê°€?", "ì—­ì‚¬ê´€ê¹Œì§€ ê±¸ì–´ì„œ ì–¼ë§ˆë‚˜ ê±¸ë ¤?"',
            'location': '"ì—­ì‚¬ê´€ ê·¼ì²˜ì— ë­ ìˆì–´?", "ë³¸ê´€ì€ ì—­ì‚¬ê´€ ì–´ëŠ ìª½ì´ì•¼?"',
            'complex': '"ì—­ì‚¬ê´€ì— ë­ ìˆê³  ì–´ë–»ê²Œ ê°€?", "ì—­ì‚¬ê´€ì´ë‘ ë³¸ê´€ ì¤‘ ì–´ë””ê°€ ë” ê°€ê¹Œì›Œ?"'
        }
    else:  # english
        lang_instruction = "ì˜ì–´ë¡œ"
        lang_note = "ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ ì˜ì–´ (informal/conversational)"
        example_questions = {
            'basic': '"Where is the History Hall?", "What is building 101?", "How many floors does the History Hall have?"',
            'route': '"How do I get to the History Hall from Aeji Gate?", "How long does it take to walk to the History Hall?"',
            'location': '"What\'s near the History Hall?", "Which direction is the Main Building from the History Hall?"',
            'complex': '"What\'s in the History Hall and how do I get there?", "Which is closer, the History Hall or the Main Building?"'
        }
    
    # ë°°ì¹˜ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompts = {
        'basic': {
            'count': 10,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 10ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ê±´ë¬¼ ì´ë¦„/ìœ„ì¹˜ ì§ˆë¬¸ (3ê°œ)
            - ê±´ë¬¼ íŠ¹ì§•/ì¸µìˆ˜/ì‹œì„¤ ì§ˆë¬¸ (4ê°œ)
            - ê±´ë¬¼ ì½”ë“œ ì§ˆë¬¸ (1ê°œ)
            - ê±´ë¬¼ ì¹´í…Œê³ ë¦¬ ì§ˆë¬¸ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['basic']}
            """
        },
        'route': {
            'count': 12,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 12ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ì˜ˆì§€ë¬¸(ì§€í•˜ì² ì—­ ì¶œêµ¬)ì—ì„œ ê°€ëŠ” ë°©ë²• (5ê°œ)
            - ì†Œìš” ì‹œê°„ ì§ˆë¬¸ (3ê°œ)
            - ê·¼ì²˜ ê±´ë¬¼ ê¸°ì¤€ ê²½ë¡œ (4ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['route']}
            """
        },
        'location': {
            'count': 10,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 10ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ì£¼ë³€ ê±´ë¬¼ ì§ˆë¬¸ (5ê°œ)
            - ìƒëŒ€ì  ìœ„ì¹˜/ë°©í–¥ ì§ˆë¬¸ (5ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['location']}
            """
        },
        'complex': {
            'count': 8,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 8ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ë³µí•© ì •ë³´ ì§ˆë¬¸ (ê±´ë¬¼ íŠ¹ì§• + ìœ„ì¹˜) (3ê°œ)
            - ë¹„êµ ì§ˆë¬¸ (2ê°œ)
            - ë¶€ì • ì§ˆë¬¸ (1ê°œ)
            - ê±´ë¬¼ëª… ë³€í˜• ì§ˆë¬¸ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['complex']}
            """
        }
    }
    
    batch_config = prompts[batch_type]
```

&nbsp; ì •í™•í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
    if language == "korean":
        prompt = f"""
ë‹¹ì‹ ì€ í•œì–‘ëŒ€í•™êµ ìº í¼ìŠ¤ ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ê±´ë¬¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒë“¤ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸-ë‹µë³€ ìŒì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.
"""

[ê±´ë¬¼ ì •ë³´]
json {building_str}
[ìƒì„± ì§€ì¹¨]
{batch_config['instruction']}

[ì¤‘ìš” ê·œì¹™]
1. ì§ˆë¬¸ì€ êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„± ({lang_note})
2. ì§ˆë¬¸ í‘œí˜„ì„ ìµœëŒ€í•œ ë‹¤ì–‘í•˜ê²Œ:
   - ê±´ë¬¼ëª… ë³€í˜•: "{building_info.get('name', '')}", "{building_info.get('building_code', '')}ê´€", ê´„í˜¸ ì•ˆ ë³„ì¹­ í™œìš©
   - ì§ˆë¬¸ í˜•ì‹: "~ì–´ë””ì•¼?", "~ì•Œë ¤ì¤˜", "~ì–´ë–»ê²Œ ê°€?", "~ë­ì•¼?", "~ì°¾ìœ¼ë ¤ë©´?" ë“±
3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ, ì£¼ì–´ì§„ ì •ë³´ì—ë§Œ ê·¼ê±°
4. ë‹µë³€ ê¸¸ì´: 50-200ì ë‚´ì™¸
5. ê° QAëŠ” ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ë‚´ìš©ì´ì–´ì•¼ í•¨ (ì¤‘ë³µ ìµœì†Œí™”)
6. route_descriptionì˜ "í˜„ ìœ„ì¹˜"ëŠ” "ì˜ˆì§€ë¬¸(í•œì–‘ëŒ€ì—­ 2ë²ˆ ì¶œêµ¬)"ì„ ì˜ë¯¸í•¨
7. nearby_buildingsì˜ ë°©í–¥ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•  ê²ƒ

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:

  json
[
  {{
    "question": "ìƒì„±ëœ ì§ˆë¬¸ 1",
    "answer": "ìƒì„±ëœ ë‹µë³€ 1",
    "type": "{batch_type}",
    "building_code": "{building_info.get('building_code', '')}"
  }},
  {{
    "question": "ìƒì„±ëœ ì§ˆë¬¸ 2",
    "answer": "ìƒì„±ëœ ë‹µë³€ 2",
    "type": "{batch_type}",
    "building_code": "{building_info.get('building_code', '')}"
  }}
]


ì •í™•íˆ {batch_config['count']}ê°œì˜ QA ìŒì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
    else:  # english
        prompt = f"""
You are a Hanyang University campus guide expert.
Based on the building information below, generate natural question-answer pairs in English that students would actually ask.

[Building Information]
json{building_str}


[Generation Guidelines]
{batch_config['instruction']}

[Important Rules]
1. Questions should be written in natural conversational style ({lang_note})
2. Vary question expressions as much as possible:
   - Building name variations: "{building_info.get('name', '')}", "Building {building_info.get('building_code', '')}", use aliases if in parentheses
   - Question formats: "Where is~?", "Can you tell me~?", "How do I get to~?", "What is~?", "How can I find~?" etc.
3. Answers should be clear and friendly, based only on the given information
4. Answer length: 50-200 characters
5. Each QA should be clearly distinct (minimize duplication)
6. The "current location" in route_description means "Aeji Gate (Exit 2 of Hanyang Univ. Station)"
7. Actively use direction information from nearby_buildings

[Output Format]
Output ONLY in the following JSON array format:

json
[
  {{
    "question": "Generated question 1",
    "answer": "Generated answer 1",
    "type": "{batch_type}",
    "building_code": "{building_info.get('building_code', '')}"
  }},
  {{
    "question": "Generated question 2",
    "answer": "Generated answer 2",
    "type": "{batch_type}",
    "building_code": "{building_info.get('building_code', '')}"
  }}
]


Generate exactly {batch_config['count']} QA pairs.
"""

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=3000
            )
            content = response.choices[0].message.content.strip()
            
            if not content:
                print(f"  âš ï¸ Empty response for {language} {batch_type} batch {batch_num} (Attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    sleep(2)
                    continue
                return []
            
            # JSON íŒŒì‹±
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', content, re.DOTALL)
            if not json_match:
                json_match = re.search(r'(\[.*?\])', content, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1)
                parsed_list = json.loads(json_str)
                
                if isinstance(parsed_list, list) and len(parsed_list) > 0:
                    print(f"  âœ“ Generated {len(parsed_list)} {language} QAs for {batch_type} batch {batch_num}")
                    return parsed_list
                else:
                    raise ValueError("Parsed JSON is empty or invalid")
            else:
                raise ValueError("No JSON array found in response")
        
        except (json.JSONDecodeError, ValueError) as e:
            print(f"  âš ï¸ Parse error for {language} {batch_type} batch {batch_num}: {e} (Attempt {attempt + 1})")
            if attempt < max_retries - 1:
                sleep(2)
        except Exception as e:
            print(f"  âŒ API error for {language} {batch_type} batch {batch_num}: {e} (Attempt {attempt + 1})")
            if attempt < max_retries - 1:
                sleep(3)
    
    return []


def generate_qa_pairs_for_building(building_info, language="korean"):
    """
    ë‹¨ì¼ ê±´ë¬¼ì— ëŒ€í•´ 40ê°œì˜ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë°°ì¹˜ë³„ë¡œ ë‚˜ëˆ ì„œ ìƒì„±í•˜ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
    """
    building_name = building_info.get('name', 'Unknown')
    lang_label = "ğŸ‡°ğŸ‡·" if language == "korean" else "ğŸ‡ºğŸ‡¸"
    print(f"\nğŸ¢ {lang_label} Processing: {building_name} ({language.upper()})")
    
    all_qa_pairs = []
    
    # ë°°ì¹˜ë³„ ìƒì„± (ì´ 40ê°œ ëª©í‘œ)
    batches = [
        ('basic', 1),      # 10ê°œ
        ('route', 1),      # 12ê°œ
        ('location', 1),   # 10ê°œ
        ('complex', 1),    # 8ê°œ
    ]
    
    for batch_type, batch_num in batches:
        qa_batch = generate_qa_batch(building_info, batch_type, batch_num, language)
        all_qa_pairs.extend(qa_batch)
        sleep(1)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
    
    print(f"  ğŸ“Š Total generated: {len(all_qa_pairs)} {language} QA pairs")
    return all_qa_pairs


def generate_all_qa_pairs(input_data):
    """
    ëª¨ë“  ê±´ë¬¼ì— ëŒ€í•´ í•œêµ­ì–´ì™€ ì˜ì–´ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    korean_qa_pairs = []
    english_qa_pairs = []
    korean_counter = 0
    english_counter = 0
    
    for building in tqdm(input_data, desc="Generating bilingual QA dataset"):
        try:
            # í•œêµ­ì–´ QA ìƒì„±
            korean_pairs = generate_qa_pairs_for_building(building, language="korean")
            
            if korean_pairs:
                for qa in korean_pairs:
                    qa_pair = {
                        "id": f"KO_QA_{korean_counter:05d}",
                        "language": "korean",
                        "question": qa.get("question", ""),
                        "answer": qa.get("answer", ""),
                        "type": qa.get("type", "unknown"),
                        "building_code": qa.get("building_code", ""),
                        "building_name": building.get("name", ""),
                        "context": building
                    }
                    korean_qa_pairs.append(qa_pair)
                    korean_counter += 1
            else:
                print(f"âš ï¸ Warning: No Korean QA pairs generated for {building.get('name', 'Unknown')}")
            
            sleep(2)  # í•œì˜ ìƒì„± ì‚¬ì´ ëŒ€ê¸°
            
            # ì˜ì–´ QA ìƒì„±
            english_pairs = generate_qa_pairs_for_building(building, language="english")
            
            if english_pairs:
                for qa in english_pairs:
                    qa_pair = {
                        "id": f"EN_QA_{english_counter:05d}",
                        "language": "english",
                        "question": qa.get("question", ""),
                        "answer": qa.get("answer", ""),
                        "type": qa.get("type", "unknown"),
                        "building_code": qa.get("building_code", ""),
                        "building_name": building.get("name", ""),
                        "context": building
                    }
                    english_qa_pairs.append(qa_pair)
                    english_counter += 1
            else:
                print(f"âš ï¸ Warning: No English QA pairs generated for {building.get('name', 'Unknown')}")
        
        except Exception as e:
            print(f"âŒ Error processing {building.get('name', 'Unknown')}: {e}")
    
    return korean_qa_pairs, english_qa_pairs


def save_qa_pairs_to_json(qa_pairs, output_path, language):
    """ìƒì„±ëœ QA ìŒ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Saved {language} QA pairs to: {output_path}")


def print_statistics(korean_qa, english_qa):
    """ìƒì„±ëœ QA ë°ì´í„°ì…‹ì˜ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*70)
    print("ğŸ“Š BILINGUAL DATASET STATISTICS")
    print("="*70)
    
    # í•œêµ­ì–´ í†µê³„
    print("\nğŸ‡°ğŸ‡· KOREAN QA PAIRS")
    print("-" * 70)
    print(f"Total: {len(korean_qa)}")
    
    ko_type_counts = {}
    for qa in korean_qa:
        qa_type = qa.get('type', 'unknown')
        ko_type_counts[qa_type] = ko_type_counts.get(qa_type, 0) + 1
    
    print("\n[By Type]")
    for qa_type, count in sorted(ko_type_counts.items()):
        print(f"  {qa_type:12s}: {count:4d} ({count/len(korean_qa)*100:.1f}%)")
    
    ko_building_counts = {}
    for qa in korean_qa:
        building = qa.get('building_name', 'Unknown')
        ko_building_counts[building] = ko_building_counts.get(building, 0) + 1
    
    print(f"\n[By Building]")
    if len(ko_building_counts) > 0:
        print(f"  Total buildings: {len(ko_building_counts)}")
        print(f"  Avg QA per building: {len(korean_qa)/len(ko_building_counts):.1f}")
        print(f"  Min: {min(ko_building_counts.values())}, Max: {max(ko_building_counts.values())}")
    
    # ì˜ì–´ í†µê³„
    print("\n" + "-" * 70)
    print("ğŸ‡ºğŸ‡¸ ENGLISH QA PAIRS")
    print("-" * 70)
    print(f"Total: {len(english_qa)}")
    
    en_type_counts = {}
    for qa in english_qa:
        qa_type = qa.get('type', 'unknown')
        en_type_counts[qa_type] = en_type_counts.get(qa_type, 0) + 1
    
    print("\n[By Type]")
    for qa_type, count in sorted(en_type_counts.items()):
        print(f"  {qa_type:12s}: {count:4d} ({count/len(english_qa)*100:.1f}%)")
    
    en_building_counts = {}
    for qa in english_qa:
        building = qa.get('building_name', 'Unknown')
        en_building_counts[building] = en_building_counts.get(building, 0) + 1
    
    print(f"\n[By Building]")
    if len(en_building_counts) > 0:
        print(f"  Total buildings: {len(en_building_counts)}")
        print(f"  Avg QA per building: {len(english_qa)/len(en_building_counts):.1f}")
        print(f"  Min: {min(en_building_counts.values())}, Max: {max(en_building_counts.values())}")
    
    # ì „ì²´ í†µê³„
    print("\n" + "-" * 70)
    print("ğŸŒ COMBINED STATISTICS")
    print("-" * 70)
    print(f"Total QA pairs (Korean + English): {len(korean_qa) + len(english_qa)}")
    print(f"Korean: {len(korean_qa)} ({len(korean_qa)/(len(korean_qa)+len(english_qa))*100:.1f}%)")
    print(f"English: {len(english_qa)} ({len(english_qa)/(len(korean_qa)+len(english_qa))*100:.1f}%)")
    
    print("="*70 + "\n")
```

### mainí•¨ìˆ˜ êµ¬ì„±ì„±
```python
if __name__ == "__main__":
    
    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    input_json_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\campus_buiding_data3.json"
    korean_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\\location_qa_pairs_korean.json"
    english_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\\location_qa_pairs_english.json"
    combined_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\\location_qa_pairs_combined.json"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(korean_output_path), exist_ok=True)
    
    try:
        # 1. ì…ë ¥ JSON íŒŒì¼ ë¡œë“œ
        input_data = load_input_json(input_json_path)
        
        # 2. í•œì˜ QA ìŒ ìƒì„±
        if isinstance(input_data, list):
            korean_qa, english_qa = generate_all_qa_pairs(input_data)
        elif isinstance(input_data, dict):
            print("Input is a single object, wrapping in a list.")
            korean_qa, english_qa = generate_all_qa_pairs([input_data])
        else:
            print("âŒ Error: Input JSON format is not a list or object.")
            korean_qa, english_qa = [], []
        
        if not korean_qa and not english_qa:
            print("âŒ No QA pairs were generated. Please check your input data and API.")
            exit(1)
        
        # 3. ìƒ˜í”Œ ì¶œë ¥
        print("\n" + "="*70)
        print("ğŸ“ SAMPLE QA PAIRS")
        print("="*70)
        
        if korean_qa:
            print("\nğŸ‡°ğŸ‡· Korean Sample (First 2):")
            for i, qa in enumerate(korean_qa[:2], 1):
                print(f"\n[KO Sample {i}]")
                print(f"Q: {qa['question']}")
                print(f"A: {qa['answer']}")
                print(f"Type: {qa['type']}, Building: {qa['building_name']}")
        
        if english_qa:
            print("\nğŸ‡ºğŸ‡¸ English Sample (First 2):")
            for i, qa in enumerate(english_qa[:2], 1):
                print(f"\n[EN Sample {i}]")
                print(f"Q: {qa['question']}")
                print(f"A: {qa['answer']}")
                print(f"Type: {qa['type']}, Building: {qa['building_name']}")
        
        print("="*70)
        
        # 4. í†µê³„ ì¶œë ¥
        print_statistics(korean_qa, english_qa)
        
        # 5. JSON íŒŒì¼ë¡œ ì €ì¥ (ë¶„ë¦¬)
        save_qa_pairs_to_json(korean_qa, korean_output_path, "Korean")
        save_qa_pairs_to_json(english_qa, english_output_path, "English")
        
        # 6. í†µí•© íŒŒì¼ë„ ì €ì¥ (ì„ íƒì‚¬í•­)
        combined_qa = korean_qa + english_qa
        save_qa_pairs_to_json(combined_qa, combined_output_path, "Combined")
        
        print("\n" + "="*70)
        print("âœ… Successfully generated bilingual QA dataset!")
        print("="*70)
        print(f"ğŸ‡°ğŸ‡· Korean QA pairs: {len(korean_qa)}")
        print(f"   Target: {len(input_data)} buildings Ã— 40 = {len(input_data) * 40}")
        print(f"   Achievement: {len(korean_qa)/(len(input_data)*40)*100:.1f}%")
        
        print(f"\nğŸ‡ºğŸ‡¸ English QA pairs: {len(english_qa)}")
        print(f"   Target: {len(input_data)} buildings Ã— 40 = {len(input_data) * 40}")
        print(f"   Achievement: {len(english_qa)/(len(input_data)*40)*100:.1f}%")
        
        print(f"\nğŸŒ Total QA pairs: {len(korean_qa) + len(english_qa)}")
        print("="*70)

    except FileNotFoundError:
        print(f"âŒ Error: Input file not found at {input_json_path}")
    except Exception as e:
        print(f"âŒ An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()
```

### clean.pyë¡œ ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±

&nbsp; ì´ ë°©ì‹ì„ ì´ìš©í•´ êµë‚´ ê±´ë¬¼ ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1kmì˜ êµì™¸ ê±´ë¬¼ê³¼ ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1~2kmì˜ ê±´ë¬¼ë„ í”„ë¡¬í”„íŠ¸ë§Œ ìˆ˜ì •í•˜ì—¬ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í›„ì• ëŠ” trainê³¼ valë°ì´í„°ë¥¼ 9:1ë¡œ ë‚˜ëˆ„ì–´ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒ 6ê°œì˜ ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í›„ëŠ” clean.pyë¥¼ í†µí•´ QAë°ì´í„°ë§Œìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ìœ¼ë©° Base-modelì´ ìš”êµ¬í•˜ëŠ” messageí˜•ì‹ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ ìµœì¢… ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

1. train_data_1km_messages.json (ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1km êµì™¸ ê±´ë¬¼ train ë°ì´í„°)
2. train_data_2km_messages.json (ì˜ˆì§€ë¬¸ ê¸°ì¤€ êµë‚´ ê±´ë¬¼ train ë°ì´í„°)
3. train_data_in_messages.json (ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1km êµì™¸ ê±´ë¬¼ train ë°ì´í„°)
4. val_data_1km_messages.json (ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1km êµì™¸ ê±´ë¬¼ val ë°ì´í„°)
5. val_data_2km_messages.json (ì˜ˆì§€ë¬¸ ê¸°ì¤€ ë°˜ê²½ 1km~2km êµì™¸ ê±´ë¬¼ val ë°ì´í„°)
6. val_data_in_messages.json (ì˜ˆì§€ë¬¸ ê¸°ì¤€ êµë‚´ ê±´ë¬¼ val ë°ì´í„°)

# Methodology 
ëŒ€ëµì ì¸ ì•Œê³ ë¦¬ì¦˜
> 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
> 2. Google Drive ë§ˆìš´íŠ¸
> 3. QLoRA í•™ìŠµ ë° LoRA ì–´ëŒ‘í„° ì €ì¥
> 4. í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ Driveì— ë°±ì—… 
> 5. ë² ì´ìŠ¤ ëª¨ë¸ ë° LoRA ì–´ëŒ‘í„°ë¡œ Merged ëª¨ë¸ ë³‘í•©
> 6. í…ŒìŠ¤íŠ¸

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```python
!pip install -q transformers accelerate bitsandbytes peft trl datasets huggingface_hub ipywidgets
```

### 2. Google Drive ë§ˆìš´íŠ¸
```python
from google.colab import drive  # Colabì—ì„œ Google Drive ì—°ê²°
drive.mount('/content/drive')
```

### 3. QLoRA í•™ìŠµ ë° LoRA ì–´ëŒ‘í„° ì €ì¥
#### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ë¡œê·¸ì¸
```python
import os                           # íŒŒì¼ / í´ë” ê²½ë¡œ
import json                         # JSON ë°ì´í„° ì½ê¸° ë° ì“°ê¸°
import random                       # ëœë¤ ì‹œë“œ ê³ ì •
import torch                        # íŒŒì´í† ì¹˜ ê¸°ë°˜ ëª¨ë¸

from transformers import (
    AutoTokenizer,                  # ìë™ìœ¼ë¡œ ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ
    AutoModelForCausalLM,           # LLMëª¨ë¸ ë¡œë“œ
    BitsAndBytesConfig,             # QLoRAìš© 4bit / 8bit ì–‘ìí™” ì„¤ì •
    TrainingArguments,              # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
    EarlyStoppingCallback,          # ì„±ëŠ¥ ìƒìŠ¹ í•œê³„ -> í•™ìŠµ ì¤‘ë‹¨
)
from peft import LoraConfig         # ê²½ëŸ‰ í•™ìŠµ ì„¤ì • ê°ì²´
from datasets import Dataset        # ë°ì´í„° -> Dataset ê°ì²´
from trl import SFTTrainer          # Supervised Fine-Tuning (SFT) 
from huggingface_hub import login   # HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

# Hugging Face ì•¡ì„¸ìŠ¤ í† í° 
HF_TOKEN = "<YOUR_HF_TOKEN>"
```

#### 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ë¡œê·¸ì¸
```python
BASE_DIR = "/content/drive/MyDrive/Gemma_2b_Fine-Tuning"            # Google Driveì— ì €ì¥ëœ íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸ ê²½ë¡œ
DATASET_DIR = BASE_DIR

QA_TRAIN_FILES = [
    os.path.join(DATASET_DIR, "train_data_1km_messages.json"),      # 1km ê¸¸ì°¾ê¸° Train ë°ì´í„°
    os.path.join(DATASET_DIR, "train_data_2km_messages.json"),      # 2km ê¸¸ì°¾ê¸° Train ë°ì´í„°
    os.path.join(DATASET_DIR, "train_data_in_messages.json"),       # í•™êµ ë‚´ ê¸¸ì°¾ê¸° Train ë°ì´í„°
]

QA_VAL_FILES = [
    os.path.join(DATASET_DIR, "val_data_1km_messages.json"),        # 1km ê¸¸ì°¾ê¸° Validation ë°ì´í„°
    os.path.join(DATASET_DIR, "val_data_2km_messages.json"),        # 1km ê¸¸ì°¾ê¸° Validation ë°ì´í„°
    os.path.join(DATASET_DIR, "val_data_in_messages.json"),         # 1km ê¸¸ì°¾ê¸° Validation ë°ì´í„°
]

MODEL_ID = "nlpai-lab/ko-gemma-2b-v1"                               # HuggingFaceì— ì˜¬ë¼ì˜¨ í•œêµ­ì–´ë¡œ Fine-tuningëœ Gemma 2B ëª¨ë¸
OUTPUT_DIR = "/content/output/gemma-2b-hanyang-guide-final"         # ì „ì²´ ëª¨ë¸ ì €ì¥í•  ê²½ë¡œ
ADAPTER_PATH = "/content/output/gemma-2b-hanyang-guide-lora-final"  # LoRA ì–´ëŒ‘í„° (LoRA ê°€ì¤‘ì¹˜) ì €ì¥í•  ê²½ë¡œ

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(ADAPTER_PATH, exist_ok=True)
```

#### 3. GPU í™•ì¸
```python
if torch.cuda.is_available():
    USE_GPU = True
else:
    USE_GPU = False
```

#### 4. QLoRA ë° LORA ì„¤ì •
```python
bnb_config = BitsAndBytesConfig(                               
    load_in_4bit=True,                                        # 4bit ì •ë°€ë„ë¡œ ëª¨ë¸ ë¡œë“œ 
    bnb_4bit_quant_type="nf4",                                # NormalFloat4 ì–‘ìí™” ì‚¬ìš© (ê¸°ì¡´ 4bit ë°©ì‹ë³´ë‹¤ ì¬í˜„ìœ¨ ë†’ìŒ -> í’ˆì§ˆ ë” ì˜ ìœ ì§€)
    bnb_4bit_compute_dtype=torch.float16,                     # ì‹¤ì œ ì—°ì‚°ì€ FP16ìœ¼ë¡œ ìˆ˜í–‰
    bnb_4bit_use_double_quant=True,                           # Double quantizationìœ¼ë¡œ, ì–‘ìí™”ëœ weightë¥¼ 1ë²ˆ ë” ì••ì¶• -> ì„±ëŠ¥ ê°ì†Œ ì—†ì´ ë” ì‘ê²Œ ì €ì¥
)

lora_config = LoraConfig(
    r=16,                                                     # LoRA Rank 
    lora_alpha=32,                                            # LoRA Scaling Factor (í†µìƒì ìœ¼ë¡œ Rankì˜ 2ë°°)
    lora_dropout=0.05,                                        # ê³¼ì í•© ë°©ì§€
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ì§€ì • (Attention ë‚´ Projectionë§Œ ì ìš©)
    bias="none",                                              # í¸í–¥ í•™ìŠµ X
    task_type="CAUSAL_LM",                                    # Casual Language Modelë¡œ í•™ìŠµ
)
```

#### 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
```python
tokenizer = AutoTokenizer.from_pretrained(                    # HuggingFaceì—ì„œ MODEL_IDì— í•´ë‹¹ë˜ëŠ” í† í¬ë‚˜ì´ì € íŒŒì¼ ë‹¤ìš´
    MODEL_ID,
    local_files_only=False,
)
tokenizer.padding_side = "right"                              # íŒ¨ë”© ë°©í–¥ ì„¤ì • (ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ íŒ¨ë”©)

model = AutoModelForCausalLM.from_pretrained(                 # HuggingFaceì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    MODEL_ID,
    quantization_config=bnb_config,                           # NF4 ê¸°ë°˜ QLoRA ì„¤ì •
    device_map="auto",                                        # GPU ë° CPU ìë™ê°ì§€
    torch_dtype=torch.float16,                                # ê³„ì‚° ì‹œ ë°ì´í„° íƒ€ì…ì„ FP16ìœ¼ë¡œ ì„¤ì •
    local_files_only=False,                                   # ë¡œì»¬ì— ëª¨ë¸ ì—†ì„ ì‹œ HuggingFaceì—ì„œ ë‹¤ìš´ 
)
```

#### 6. ë°ì´í„°ì…‹ ë¡œë“œ (message í¬ë§·)
```python
def load_messages_data(file_paths, dataset_type="Train"):                          # JSON íŒŒì¼ë“¤ë¡œ ë‚´ë¶€ ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    all_texts = []                                                                 # ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì ì‹œí‚¬ ë¦¬ìŠ¤íŠ¸
    for file_path in file_paths:
        if not os.path.exists(file_path):
            continue
        try:
            with open(file_path, "r", encoding="utf-8") as f:                      # UTF-8ë¡œ JSON íŒŒì¼ ì½ê³ , ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                data = json.load(f)
            if isinstance(data, list):
                for item in data:
                    if "messages" in item and isinstance(item["messages"], list):
                        try:
                            text = tokenizer.apply_chat_template(                  # ëŒ€í™”í˜• í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (message ë¦¬ìŠ¤íŠ¸ -> í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ì‚¬ìš©)
                                item["messages"],
                                tokenize=False,
                                add_generation_prompt=False,
                            )
                            all_texts.append(text)
                        except Exception as e:
                            pass
        except Exception as e:
            pass
    return all_texts

train_texts = load_messages_data(QA_TRAIN_FILES, "Train")                          # Trainìš© JSON 3ê°œ íŒŒì¼ì„ ì½ì€ ë’¤, í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±
val_texts = load_messages_data(QA_VAL_FILES, "Validation")                         # Validationìš© JSON 3ê°œ íŒŒì¼ì„ ì½ì€ ë’¤, í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±

if not val_texts and train_texts:                                                  # Validation JSONì´ ì—†ëŠ” ê²½ìš°, Train dataë¥¼ 90/10ìœ¼ë¡œ Split ì‹œí‚¤ê¸°
    split_idx = int(len(train_texts) * 0.9)
    val_texts = train_texts[split_idx:]
    train_texts = train_texts[:split_idx]

train_dataset = Dataset.from_dict({"text": train_texts}) if train_texts else Dataset.from_dict({"text": []})   # SFTTrainerìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
eval_dataset = Dataset.from_dict({"text": val_texts}) if val_texts else Dataset.from_dict({"text": []})
```

#### 7. formatting_func ì •ì˜
```python
def formatting_func(example):   # ë°ì´í„°ì…‹ì˜ 1ê°œì˜ ìƒ˜í”Œì„ ë°›ì•„, ëª¨ë¸ì— ë„˜ê¸¸ í˜•íƒœë¡œ ê°€ê³µ
    return example["text"]
```



#### 8. SFTTrainer ì„¤ì •
```python
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,              # í•™ìŠµê²°ê³¼ ì €ì¥í•  í´ë”
    num_train_epochs=3,                 # Epoch
    per_device_train_batch_size=2,      # GPU 1ê°œë‹¹ 2ê°œ ìƒ˜í”Œì”© í•™ìŠµì— ë„£ìŒ (Train)
    per_device_eval_batch_size=2,       # GPU 1ê°œë‹¹ 2ê°œ ìƒ˜í”Œì”© í•™ìŠµì— ë„£ìŒ (Validation)
    gradient_accumulation_steps=8,      # 8ë²ˆ ëˆ„ì í•œ ë’¤ ì—…ë°ì´íŠ¸
    gradient_checkpointing=True,        # ì¤‘ê°„ ê³„ì‚°ì„ ì €ì¥í•˜ì§€ ì•Šê³  ì—­ì „íŒŒ ë•Œ ë‹¤ì‹œ ê³„ì‚° -> ë©”ëª¨ë¦¬ ì ˆì•½
    max_grad_norm=1.0,                  # Gradient Clipping ë°©ì§€
    optim="paged_adamw_8bit",           # ì˜µí‹°ë§ˆì´ì € (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê·œì¹™) ì„¤ì • / paged_adamw_8bit
    learning_rate=2e-4,                 # í•™ìŠµë¥ 
    lr_scheduler_type="cosine",         # Scheduler (í•™ìŠµë¥  ì¤„ì´ëŠ” í•¨ìˆ˜)ë¥¼ Cosineë¡œ ì„¤ì •
    warmup_ratio=0.03,                  # ì „ì²´ ìŠ¤í…ì˜ 3% êµ¬ê°„ê¹Œì§€ëŠ” í•™ìŠµë¥  0ìœ¼ë¡œ ì„¤ì •
    weight_decay=0.01,                  # ê³¼ì í•© ë°©ì§€
    eval_strategy="steps",              
    eval_steps=100,                     # 100 Stepë§ˆë‹¤ Validation ì‹¤í–‰
    save_steps=100,                     # 100 Stepë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    save_total_limit=3,                 # ê°€ì¥ ìµœê·¼ì˜ ì²´í¬í¬ì¸íŠ¸ 3ê°œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì‚­ì œ
    fp16=True,                          # í•™ìŠµì„ Float16ìœ¼ë¡œ ìˆ˜í–‰
    bf16=False,                         
    load_best_model_at_end=True,        # í•™ìŠµ ì¢…ë£Œ í›„ Validationì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    metric_for_best_model="eval_loss",  # Eval_Lossë¡œ ëª¨ë¸ í‰ê°€ ê¸°ì¤€ ì„¤ì •
    greater_is_better=False,
    logging_dir=f"{OUTPUT_DIR}/logs",   # ë¡œê·¸ ì €ì¥ í´ë”
    logging_steps=10,                   # 10 Stepë§ˆë‹¤ í•™ìŠµ ë¡œê·¸ ì¶œë ¥
    report_to="tensorboard",            # Tensorboardì— ë¡œê·¸ ê¸°ë¡
)

early_stopping = EarlyStoppingCallback(early_stopping_patience=3)              # Eval_Lossê°€ 3ë²ˆ ì´ìƒ í–¥ìƒí•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ í•™ìŠµ ì¤‘ë‹¨

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    formatting_func=formatting_func,
    callbacks=[early_stopping],
)
```




#### 9. Training
```python
trainer.train()                                  # ìœ„ì˜ ì„¤ì • ë”°ë¼ í•™ìŠµ
try:
    trainer.model.save_pretrained(ADAPTER_PATH)  # LoRA ê°€ì¤‘ì¹˜ ì €ì¥
    tokenizer.save_pretrained(ADAPTER_PATH)      # í† í¬ë‚˜ì´ì € ì €ì¥ (í† í°í™”ëœ ë¬¸ì¥)
```


### 4. í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ Driveì— ë°±ì—…
```python
!mkdir -p /content/drive/MyDrive/Gemma_2B_Trained                                                   # Google Driveì— Gemma_2B_Trained í´ë” ìƒì„±
!cp -r /content/output/gemma-2b-hanyang-guide-lora-final /content/drive/MyDrive/Gemma_2B_Trained/   # LoRA Adapter ë° í† í¬ë‚˜ì´ì € ì €ì¥
```


### 5. ë² ì´ìŠ¤ ëª¨ë¸ ë° LoRA ì–´ëŒ‘í„° Merged ëª¨ë¸ ë³‘í•©
```python
os.makedirs(OUTPUT_DIR, exist_ok=True)
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,                     # Epoch
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    eval_strategy="steps",
    eval_steps=0.2,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=10,
    warmup_steps=5,
    logging_strategy="steps",
    learning_rate=2e-4,                     # í•™ìŠµë¥ 
    fp16=True,
    report_to="tensorboard",
    save_strategy="epoch",
    load_best_model_at_end=False,
)
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    formatting_func=lambda x: x["text"],
)
```

### 6. í•™ìŠµ ì‹¤í–‰ ë° LoRA ì–´ëŒ‘í„° ì €ì¥, ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©
#### 1. ê²½ë¡œ ì„¤ì •
```python
BASE_MODEL = "nlpai-lab/ko-gemma-2b-v1"

# â¬‡â¬‡â¬‡ ì—¬ê¸° ë‘ ì¤„ë§Œ ë„¤ ë“œë¼ì´ë¸Œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì¤˜ â¬‡â¬‡â¬‡
ADAPTER_PATH = "/content/drive/MyDrive/Gemma_2b_Fine-Tuning/gemma-2b-hanyang-guide-lora-final"
MERGED_PATH  = "/content/drive/MyDrive/Gemma_2b_Fine-Tuning/gemma-2b-hanyang-final-merged"
# â¬†â¬†â¬† í´ë” ì´ë¦„/ê²½ë¡œë§Œ ì •í™•íˆ ë§ì¶”ë©´ ë¨ â¬†â¬†â¬†

# ì–´ëŒ‘í„° ê²½ë¡œ í™•ì¸
if not os.path.exists(ADAPTER_PATH):
    raise FileNotFoundError(f"âŒ ì–´ëŒ‘í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ADAPTER_PATH}")

os.makedirs(MERGED_PATH, exist_ok=True)

print("=" * 70)
print("ğŸ”„ Gemma LoRA â†’ Merged ëª¨ë¸ ë³‘í•© (Colab/GPU ë²„ì „)")
print("=" * 70)
print(f"ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸: {BASE_MODEL}")
print(f"ğŸ”— LoRA ì–´ëŒ‘í„°: {ADAPTER_PATH}")
print(f"ğŸ’¾ ë³‘í•© ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MERGED_PATH}")
print("=" * 70 + "\n")
```

#### 2. ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ì •ë³´
```python
if torch.cuda.is_available():
    device = "cuda"
    print(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
else:
    device = "cpu"
    print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ë³‘í•© ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±(OOM)ì´ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

print()
```

#### 3. ë² ì´ìŠ¤ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
```python
print("1ë‹¨ê³„: ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",          # GPU ìë™ ì‚¬ìš©
    torch_dtype=torch.float16,  # fp16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
)

print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")

print("2ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
# ì–´ëŒ‘í„° ìª½ì— ì €ì¥ëœ tokenizerë¥¼ ìš°ì„  ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    base_model.resize_token_embeddings(len(tokenizer))
    print("   âš ï¸ pad_tokenì´ ì—†ì–´ ìƒˆë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n")
```

#### 4. LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
```python
print("3ë‹¨ê³„: LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘...")
model = PeftModel.from_pretrained(
    base_model,
    ADAPTER_PATH,
    device_map="auto"
)
print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ\n")

print("4ë‹¨ê³„: merge_and_unload() ì‹¤í–‰ ì¤‘...")
merged_model = model.merge_and_unload()   # LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ì— êµ½ê¸°
merged_model.to(device)
print("âœ… merge_and_unload() ì„±ê³µ\n")

# (ì„ íƒ) PEFT ê´€ë ¨ ì†ì„± ì •ë¦¬ - ê¼­ ì—†ì–´ë„ ë˜ì§€ë§Œ ê¹”ë”í•˜ê²Œ ì •ë¦¬
attrs_to_remove = [
    'peft_config',
    'active_adapter',
    'active_adapters',
    '_hf_peft_config_loaded',
    'peft_type',
    'base_model_prefix'
]
for attr in attrs_to_remove:
    if hasattr(merged_model, attr):
        try:
            delattr(merged_model, attr)
            print(f"   âœ“ {attr} ì œê±°ë¨")
        except Exception:
            pass

print("âœ… ì†ì„± ì •ë¦¬ ì™„ë£Œ\n")
```

#### 5. ë³‘í•© ëª¨ë¸ ì €ì¥
```python
print("5ë‹¨ê³„: ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")

try:
    merged_model.save_pretrained(
        MERGED_PATH,
        safe_serialization=True,   # safetensorsë¡œ ì €ì¥
        max_shard_size="2GB",
    )
    tokenizer.save_pretrained(MERGED_PATH)
    print(f"âœ… ë³‘í•© ëª¨ë¸ì´ {MERGED_PATH} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
except Exception as e:
    print(f"âš ï¸ safe_serialization ë°©ì‹ ì €ì¥ ì‹¤íŒ¨: {e}")
    print("   â†’ PyTorch ê¸°ë³¸ í¬ë§·ìœ¼ë¡œ ë‹¤ì‹œ ì €ì¥ ì‹œë„...")
    merged_model.save_pretrained(
        MERGED_PATH,
        safe_serialization=False,
        max_shard_size="2GB",
    )
    tokenizer.save_pretrained(MERGED_PATH)
    print(f"âœ… PyTorch í¬ë§·ìœ¼ë¡œ {MERGED_PATH} ì— ì €ì¥ ì™„ë£Œ!\n")

print("=" * 70)
print("âœ… ëª¨ë¸ ë³‘í•© ì™„ë£Œ (Colab)")
print("=" * 70)
```

#### 6. ê²€ì¦
```python
print("\n" + "=" * 70)
print("ğŸ§ª ì €ì¥ëœ Merged ëª¨ë¸ ê²€ì¦ (ê°„ë‹¨ í…ŒìŠ¤íŠ¸)")
print("=" * 70)

try:
    test_tokenizer = AutoTokenizer.from_pretrained(MERGED_PATH)
    test_model = AutoModelForCausalLM.from_pretrained(
        MERGED_PATH,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    test_model.eval()
    print("âœ… ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n")

    from textwrap import shorten

    test_questions = [
        "í•œì–‘ëŒ€í•™êµ ERICA ì •ë¬¸ì—ì„œ ì œ2ê³µí•™ê´€ê¹Œì§€ ì–´ë–»ê²Œ ê°€?",
        "ì–´ë””ì—ì„œ í•™ìƒíšŒê´€(í•™ìƒíšŒê´€ ê±´ë¬¼)ì„ ì°¾ì„ ìˆ˜ ìˆì–´?"
    ]

    for i, q in enumerate(test_questions, 1):
        print(f"[í…ŒìŠ¤íŠ¸ {i}] Q: {q}")
        messages = [{"role": "user", "content": q}]
        prompt = test_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = test_tokenizer(prompt, return_tensors="pt").to(test_model.device)

        with torch.no_grad():
            out_ids = test_model.generate(
                **inputs,
                max_new_tokens=64,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=test_tokenizer.eos_token_id,
            )

        gen_ids = out_ids[0][inputs["input_ids"].shape[-1]:]
        ans = test_tokenizer.decode(gen_ids, skip_special_tokens=True).strip()
        print("A:", shorten(ans, width=150, placeholder="..."))
        print("-" * 70)

    print("\nâœ… ê°„ë‹¨ ì¶”ë¡  í…ŒìŠ¤íŠ¸ê¹Œì§€ ì™„ë£Œ!")

except Exception as e:
    print(f"âš ï¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("   (ê·¸ë˜ë„ ë³‘í•© ëª¨ë¸ íŒŒì¼ì€ MERGED_PATHì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.)")

print("\nìµœì¢… ì €ì¥ ê²½ë¡œ:", MERGED_PATH)
print("=" * 70)
```

### 7. í…ŒìŠ¤íŠ¸
#### 1. ë³‘í•© ëª¨ë¸ ê²½ë¡œ ì„¤ì •
```python
MERGED_MODEL_PATH = "/content/drive/MyDrive/Gemma_2b_Merged"
```

#### 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
```python
if torch.cuda.is_available():
    device = "cuda"
    print(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
else:
    device = "cpu"
    print("âš ï¸ GPU ì—†ìŒ, CPUë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤. (ì†ë„ ëŠë¦´ ìˆ˜ ìˆìŒ)")
print()
```

#### 3. í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
```python
print("ğŸ“¦ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)
print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
print(f"   BOS: {repr(tokenizer.bos_token)} (ID: {tokenizer.bos_token_id})")
print(f"   EOS: {repr(tokenizer.eos_token)} (ID: {tokenizer.eos_token_id})")
print(f"   PAD: {repr(tokenizer.pad_token)} (ID: {tokenizer.pad_token_id})")
print(f"   Chat template ì¡´ì¬ ì—¬ë¶€: {tokenizer.chat_template is not None}")
print()

print("ğŸ“¦ ë³‘í•©ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MERGED_MODEL_PATH,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,  # GPU ìˆìœ¼ë©´ ìë™, ì—†ìœ¼ë©´ CPU
)
model.eval()

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"   ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
print("=" * 70 + "\n")
```

#### 4. ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (chat template)
```python
def hanyang_guide_chat(
    user_query: str,
    history=None,
    max_new_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repetition_penalty: float = 1.1,
):
    """
    ë³‘í•©ëœ Ko-Gemma í•œì–‘ ê¸¸ì•ˆë‚´ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±.
    ko-gemma chat_templateì€ system roleì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
    system_promptë¥¼ ì²« user ë°œí™”ì— í…ìŠ¤íŠ¸ë¡œ í¬í•¨ì‹œí‚¤ëŠ” ë°©ì‹ ì‚¬ìš©.
    """
    if history is None:
        history = []

    # ì›ë˜ systemìœ¼ë¡œ ë„£ê³  ì‹¶ë˜ ì§€ì¹¨ì„ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë¡œ í¬í•¨
    system_prompt = (
        "ë‹¹ì‹ ì€ í•œì–‘ëŒ€í•™êµ(ì„œìš¸/ERICA í¬í•¨)ì˜ ê¸¸ì•ˆë‚´ì™€ ê±´ë¬¼, ì‹œì„¤ ì •ë³´ë¥¼ ë„ì™€ì£¼ëŠ” AIì…ë‹ˆë‹¤. "
        "ëª¨ë¥´ëŠ” ì •ë³´ëŠ” ì§€ì–´ë‚´ì§€ ë§ê³  'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”. "
        "ê¸¸ì„ ì„¤ëª…í•  ë•ŒëŠ” ëœë“œë§ˆí¬ë¥¼ í™œìš©í•´ì„œ ì°¨ë¶„í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."
    )

    messages = []

    # ê³¼ê±° ëŒ€í™” ë³µì› (ko-gemma í…œí”Œë¦¿ì€ user / assistant ì¡°í•©ì„ ì§€ì›)
    for u, a in history:
        messages.append({"role": "user", "content": u})
        messages.append({"role": "assistant", "content": a})

    # ì´ë²ˆ ì§ˆë¬¸: system_promptë¥¼ ì•ì— ë¶™ì—¬ì„œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¤Œ
    full_user_content = system_prompt + "\n\n" + user_query
    messages.append({"role": "user", "content": full_user_content})

    # Gemma chat template ì ìš©
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,  # ë§ˆì§€ë§‰ì— <start_of_turn>model\n ì¶”ê°€
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    gen_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

    return answer
```

#### 5. ê°„ë‹¨ í…ŒìŠ¤íŠ¸
```python
test_questions = [
    "í•œì–‘ëŒ€í•™êµ ERICA ì •ë¬¸ì—ì„œ ì œ2ê³µí•™ê´€ê¹Œì§€ ì–´ë–»ê²Œ ê°€ì•¼ í•´?",
    "ì œ2ê³µí•™ê´€ ê·¼ì²˜ì— í¸ì˜ì ì´ë‚˜ ì¹´í˜ ìˆì–´?",
]

print("=" * 70)
print("ğŸ§ª ê°„ë‹¨ í…ŒìŠ¤íŠ¸")
print("=" * 70)

for i, q in enumerate(test_questions, 1):
    print(f"\n[ì§ˆë¬¸ {i}] {q}")
    ans = hanyang_guide_chat(q)
    print(f"[ë‹µë³€] {ans}")
    print("-" * 70)

print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ. ì´ì œ hanyang_guide_chat(ì§ˆë¬¸) ìœ¼ë¡œ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµìƒ
```

# 5. Evaluation & Analysis
&nbsp; ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ í›ˆë ¨ì‹œí‚¨ QAë°ì´í„° ì¤‘ì—ì„œ ë¬´ì‘ìœ„ì ìœ¼ë¡œ 3ê°œì˜ ì§ˆë¬¸ì„ ì„ íƒí•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì€ ê°„ë‹¨í•œ í•œêµ­ì–´ ì§ˆë¬¸ 1ê°œì™€ ë³µì¡í•œ í•œêµ­ì–´ ì§ˆë¬¸ 1ê°œ, ì˜ì–´ ì§ˆë¬¸ 1ê°œë¥¼ ì„ íƒí•˜ì—¬ ì§„í–‰í•˜ì˜€ìœ¼ë©° ê²°ê³¼ëŠ” ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ìŠµë‹ˆë‹¤.

- ì›ë³¸ ë°ì´í„°ì—ì„œ ì§ˆë¬¸ê³¼ ì‘ë‹µ
1.
ì…ë ¥ : "í•œì–‘í”Œë¼ì ê±´ë¬¼ ë²ˆí˜¸ê°€ ë­ì•¼?"

ì •ë‹µ : "í•œì–‘í”Œë¼ìì˜ ê±´ë¬¼ ë²ˆí˜¸ëŠ” 105ë²ˆì´ì•¼."

2.
ì…ë ¥ : "ì—¬ê¸°ì„œ ë¹„íŠ¸í”Œë ‰ìŠ¤ì•½êµ­ ì–´ë–»ê²Œ ê°€?"

ì •ë‹µ : "í•œì–‘ëŒ€ì—­ 2ë²ˆ ì¶œêµ¬ë¡œ ë‚˜ì™€ì„œ, 1ë²ˆ ì¶œêµ¬ ë°©í–¥ìœ¼ë¡œ ê¸¸ì„ ê±´ë„ˆì„¸ìš”. ê·¸ í›„ ì™•ì‹­ë¦¬ì—­ ë°©í–¥ìœ¼ë¡œ ì•½ 650më¥¼ ì§ì§„í•˜ë©´ 'ì™•ì‹­ë¦¬ì—­(ë¹„íŠ¸í”Œë ‰ìŠ¤)' ê±´ë¬¼ 2ì¸µì— ìœ„ì¹˜í•´ ìˆì–´ìš”. ë„ë³´ë¡œ ì•½ 14ë¶„ ì •ë„ ê±¸ë ¤ìš”."

3.
ì…ë ¥ : "What's near the Natural Sciences Building?"

ì •ë‹µ : "The Natural Sciences Building is near the Humanities Hall above and the College of Education below."

- ëª¨ë¸ ê²°ê³¼ ì‚¬ì§„

<img width="928" height="264" alt="image" src="https://github.com/user-attachments/assets/411bbe45-bd81-4d36-9a6e-436be2f73489" />

í•˜ì§€ë§Œ ëª¨ë¸ì´ ë•Œë¡œëŠ” í•™ìŠµëœ ì •ë³´ê°€ ì•„ë‹Œ ë¬´ì‘ìœ„ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ì „ì˜ 10ê°œì˜ ì§ˆë¬¸ê³¼ ëŒ€ë‹µì„ ê¸°ì–µí•˜ëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‹¤í—˜í•˜ì˜€ìŠµë‹ˆë‹¤.

- ëª¨ë¸ ê¸°ì–µ ê¸°ëŠ¥ ê²°ê³¼ ì‚¬ì§„

<img width="1224" height="486" alt="image (6)" src="https://github.com/user-attachments/assets/4717a784-7d4a-4407-a1c9-bddfbdd4ab6f" />

ê²°ê³¼ë¥¼ í†µí•´ íŠ¹ì •í•œ ì§ˆë¬¸ì—ëŠ” ì •í™•í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ê¸°ì–µ ê¸°ëŠ¥ë„ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

# 6. direction for improvement
&nbsp; ìœ„ì˜ ê²°ê³¼ì™€ ê°™ì´ ëª¨ë¸ì´ í•™ìŠµí•œ ëŒ€ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµí•˜ì§€ ì•Šì€ ë‹¤ì–‘í•œ ì§ˆë¬¸ì—ë„ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ì´ íŠ¹ì •í•œ ì†Œìˆ˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ ì œëŒ€ë¡œëœ ì‘ë‹µì„ ì œì‹œí•˜ì˜€ìœ¼ë©°, ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„  ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ë˜ëŠ” ê´€ë ¨ ì—†ëŠ” ëŒ€ë‹µì„ ì œì‹œí•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ë•Œë•Œë¡œ íŠ¹ì • ì‘ë‹µì„ ê³„ì† ë°˜ë³µí•˜ì—¬ ì œì‹œí•˜ê¸°ë„ í•˜ì˜€ìœ¼ë©° í•™ìŠµëœ ë°ì´í„°ì™€ ê´€ë ¨ ì—†ëŠ” ì™„ì „í•œ ì˜¤ë¥˜ì˜ ì‘ë‹µì„ ì œì‹œí•˜ëŠ” ë“± ì„±ëŠ¥ ë©´ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê¸°ì—ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì ì„ ë¶„ì„í•˜ê³  ì´ì— ëŒ€í•œ ê°œì„ ë°©ì•ˆì„ ì œì‹œí•˜ë ¤ê³  í•©ë‹ˆë‹¤.

### 1. ë¶€ì •í™•í•œ ë°ì´í„° ë° ë°ì´í„° ìì²´ì—ì„œ ì˜¤ë¥˜ê°€ ì¡´ì¬, ì£¼ë¡œ ì§§ì€ ë‹µë³€ ë°ì´í„°ë¡œ êµ¬ì„±
   
&nbsp; ë°ì´í„°ì…‹ ìì²´ì—ì„œ ë¶€ì •í™•í•œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©°, ì˜ì–´ë¡œ ë²ˆì—­ëœ ë°ì´í„°ì—ì„œ ê±´ë¬¼ì˜ ì´ë¦„ì€ ì˜ì–´ë¡œ ë²ˆì—­ë˜ì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, ìƒì„±í•œ QAë°ì´í„°ì—ì„œ ì‘ë‹µì´ 20~50í† í°ìœ¼ë¡œ ë§¤ìš° ì§§ë‹¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬í•˜ì˜€ìœ¼ë©°, ì¡°ì‚¬ ê²°ê³¼ 11ê°œì˜ ë¬¸ì¥ì€ 20í† í° ì´í•˜ë¡œ êµ¬ì„±ëœ ê²ƒìœ¼ë¡œ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¡œ ì¸í•´ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì´ ì œì‹œë˜ë©° í•™ìŠµì— ì˜¤ë¥˜ê°€ ìƒê²¼ì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë¹„ìš©ì  ë¬¸ì œë¡œ ì¸í•´ OPENAI APIì—ì„œ gpt-4o-miniëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ë†’ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ìƒìš”í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì— ì§§ê³  ë¶€ì •í™•í•œ ëŒ€ë‹µì„ ìƒì„±í•  ìˆ˜ ë°–ì— ì—†ì—ˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

 ê°œì„  ë°©ì•ˆ : ì§€ê¸ˆì˜ ë°ì´í„°ëŠ” ê±´ë¬¼ í•˜ë‚˜ë‹¹ 40ê°œì˜ QAë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì˜€ì§€ë§Œ ì´ë¥¼ ê±´ë¬¼ í•˜ë‚˜ë‹¹ 10~20ê°œì˜ QAë°ì´í„°ì…‹ìœ¼ë¡œ ì¶•ì†Œí•˜ê³  QAë°ì´í„°ì˜ ì‘ë‹µì˜ ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ê³  ì—¬ëŸ¬ ë°ì´í„°ë¡œ êµ¬ì„±ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ì œì‹œí•˜ê³  í•™ìŠµ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ê²½ë¡œë„ ì§€ê¸ˆì€ í•˜ë‚˜ì˜ ê²½ë¡œë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆê³  ê±´ë¬¼ì˜ íŠ¹ì„±ë„ í•œì •ë˜ì–´ ìˆì§€ë§Œ ì´í›„ì— ê±´ë¬¼ì˜ ë‹¤ì–‘í•œ íŠ¹ì„±ê³¼ ê²½ë¡œë¥¼ ì¶”ê°€í•˜ì—¬ ì •í™•í•œ ê¸¸ì•ˆë‚´ ëª¨ë¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ë©´ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2. GPUìì›ì˜ í•œê³„ë¡œ ì¸í•œ ë¶€ì •í™•í•œ í•™ìŠµ ë° ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ë°œíœ˜ ë¶ˆê°€
   
&nbsp; GPUì˜ RAMìš©ëŸ‰ì´ Localí•™ìŠµì—ì„  RTX 3060Ti(8GB), ì½”ë©ì—ì„œ T4(15GB)ë¡œ LLMíŒŒì¸íŠœë‹ì— ë¹„í•´ ì‘ì€ ë©”ëª¨ë¦¬ ìš©ëŸ‰ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì œëŒ€ë¡œëœ í•™ìŠµì„ ì§„í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. QLoRAì—ì„œ ì›ë³¸ ëª¨ë¸ì„ 4ë¹„íŠ¸ ì–‘ìí™”í•˜ì—¬ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, LoRA Rankë„ r=16ìœ¼ë¡œ êµ‰ì¥íˆ ì‘ì€ ìˆ˜ì˜ Layerë§Œ ì„ íƒí•˜ì—¬ í•™ìŠµí•˜ì˜€ê¸° ë•Œë¬¸ì— ì œëŒ€ë¡œëœ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•˜ì˜€ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤. ë˜í•œ, ì›ë³¸ ëª¨ë¸ê³¼ì˜ mergeë¥¼ ì§„í–‰í–ˆì§€ë§Œ, íŒŒì¸íŠœë‹ ê³¼ì •ì—ì„œ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”ëœ ì›ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ê¸° ë•Œë¬¸ì— mergeê³¼ì •ì—ì„œ ì›ë³¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ì¶©ë¶„íˆ ë³‘í•©ë˜ì§€ ëª»í•˜ê³  ì›ë³¸ ëª¨ë¸ì˜ ê¸°ëŠ¥ì„ ìƒì‹¤í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì›ë³¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ ëª»í•˜ëŠ” ë§ê° í˜„ìƒì´ ë°œìƒí•˜ì˜€ê³  ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ì´ ì¼ìƒì ì¸ ì§ˆë¬¸ì—ëŠ” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ì›ë³¸ ëª¨ë¸ ë§ê° í˜„ìƒ
<img width="950" height="290" alt="image (7)" src="https://github.com/user-attachments/assets/0f8e502d-980b-43e0-b0ca-fef582abb3ae" />

 ê°œì„  ë°©ì•ˆ : GPUìì› í™•ë³´ë¥¼ í†µí•´ QLoRAì—ì„œ 16ë¹„íŠ¸ ë˜ëŠ” QLoRAëŒ€ì‹  LoRAë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ë©´ í•™ìŠµ ì •í™•ë„ì™€ mergeê³¼ì •ì—ì„œ ì›ë³¸ ëª¨ë¸ì˜ ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ LoRA Rankë¥¼ ë³´ë‹¤ í¬ê²Œ ì„¤ì •í•˜ì—¬ ì •í™•í•œ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤ê³  ê¸°ëŒ€ë©ë‹ˆë‹¤.




# LLMì˜ ì„±ëŠ¥ í‰ê°€ ê¸°ì¤€/ë°©ì‹
## 1. Intrinsic / Extrinsic Evaluation : ëª¨ë¸ì´ ì–¸ì–´ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‰ê°€
### - perplextiy
#### <img width="172" height="42" alt="image" src="https://github.com/user-attachments/assets/6155d5ed-3fab-4560-9378-0f369d9841b3" />
#### Nì€ ì´ í† í° ìˆ˜ p(x_i)ëŠ” x_ië²ˆì§¸ ì •ë‹µ í† í°ì„ ì´ ëª¨ë¸ì´ ë§ì¶œ í™•ë¥ 
#### í‰ê· ì ìœ¼ë¡œ ëª¨ë¸ì´ ì •ë‹µ í† í°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ë‚®ì€ í˜¼ë€ë„ë¥¼ ê°–ëŠ”ì§€ ê³„ì‚°í•¨

## 2. Task-based Evaluation ëª¨ë¸ì˜ ì‹¤ì œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ
### - MMLU
#### 57ê°œ ë¶„ì•¼ì˜ ì‹œí—˜ ë¬¸ì œì˜ ì •í™•ë„ë¥¼ í‰ê°€
#### ì‚¬ëŒ / GPT-4ë“±ì˜ ìˆ˜ì¤€ ë¹„êµì— ì‚¬ìš©

### - GSM8K
#### Grade-school math ë¬¸ì œ í’€ì´ ì •í™•ë„ë¥¼ í‰ê°€
#### LLMì˜ ìˆ˜í•™ì  ì¶”ë¡ ì„ ì§ì ‘ì ìœ¼ë¡œ í‰ê°€í•¨

### - ARC / HellaSwag / WinoGrande ë“±...

## 3. Safty / Alignment Evaluation
### - Hallucination Rate
#### ëª¨ë¸ì˜ ì¶œë ¥ì´ ì‚¬ì‹¤ê³¼ ë§ì§€ ì•Šì„ ë•Œì˜ ë¹„ìœ¨
#### ì˜¤ë¥˜ ì‘ë‹µ ìˆ˜ / ì „ì²´ ì‘ë‹µ ìˆ˜

## 4. Text Genration Quality
### - BLEU
#### n-gram precision ê¸°ë°˜
#### <img width="172" height="42" alt="518252181-4e280d64-cc25-4b3b-a31b-2fcd956d9266" src="https://github.com/user-attachments/assets/b5694356-043b-4605-8d55-d306905199de" />
#### p_nì€ ì˜ˆì¸¡ ë¬¸ì¥ê³¼ ì°¸ì¡° ë¬¸ì¥ì—ì„œ ì¼ì¹˜í•œ ìˆ˜ / ì˜ˆì¸¡ë¬¸ì¥ì˜ ì „ì²´ n-gram ìˆ˜ / w_nì€ ê°€ì¤‘ì¹˜

## 5. System-level Evaluation

# Related Work (e.g., existing studies)
#### Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Supryadi, Yu, L., Liu, Y., Li, J., Xiong, B., & Xiong, D. (2023, November 25). Evaluating large language models: A comprehensive survey (arXiv pre-print arXiv:2310.19736).
#### ê°•ë´‰ì¤€, & ê¹€ì˜ì¤€. (2025). êµ­ë‚´ ë²•ë¥  LLMì˜ í™œìš©ê³¼ ì—°êµ¬ë™í–¥ : í™˜ê°ê³¼ ë³´ì•ˆ ë¦¬ìŠ¤í¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ. ì‚°ì—…ê¸°ìˆ ì—°êµ¬ë…¼ë¬¸ì§€, 30(3), 227-240.


# Model use
&nbsp; ì œì‘ëœ ëª¨ë¸ì€ í•œì–‘í•™ìˆ íƒ€ìš´ í”„ë¡œì íŠ¸ì˜ ì¼í™˜ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, ì´ì— ëŒ€í•´ ê°„ë‹¨íˆ ì†Œê°œí•˜ë ¤ê³  í•©ë‹ˆë‹¤.

1. í•™ìˆ íƒ€ìš´ E2Eëª¨ë¸ ê°¸ìš”
2. ì–‘ìí™” 
3. ë³´ë“œ íƒ‘ì¬ 
4. ê²°ê³¼ 

### í•™ìˆ íƒ€ìš´ E2Eëª¨ë¸ ê°œìš”
&nbsp; ì œì‘ëœ ëª¨ë¸ì€ í•™ìˆ íƒ€ìš´ í”„ë¡œì íŠ¸ë¡œ ì§„í–‰í–ˆì—ˆë˜ Whisperëª¨ë¸ê³¼ TTSëª¨ë¸ì„ ê²°í•©íˆì—¬ ìŒì„± ì…ë ¥ì—ì„œ ìŒì„± ì¶œë ¥ìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” í•˜ë‚˜ì˜ E2Eëª¨ë¸ë¡œ êµ¬ì„±í•˜ëŠ”ë° ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. WhisperëŠ” ASRëª¨ë¸ë¡œ ìŒì„± ì…ë ¥ì„ íŠ¹ì • ì–¸ì–´ë¡œ ë²ˆì—­í•´ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, í”„ë¡œì íŠ¸ì—ì„  í•œêµ­ì–´ ì…ë ¥ì„ ë°›ê¸° ë” ì •í™•íˆ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œ í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì˜€ìœ¼ë©°, ì‘ë™ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. Whisperëª¨ë¸ê³¼ VADí–‰

<img width="386" height="209" alt="image" src="https://github.com/user-attachments/assets/9eb82b99-3d48-479e-b26a-05a77039cd18" />

- ì–‘ìí™” í›„ ìš©ëŸ‰ ë¹„êµ

<img width="593" height="588" alt="image" src="https://github.com/user-attachments/assets/e15b60e4-d3ab-420a-947c-be7fe5814227" />


- ì–‘ìí™” í›„ ê²°ê³¼ ë¹„êµ
  
> ì›ë³¸ëª¨ë¸(Gemma-2b-hanyang-final-merged)

<img width="928" height="264" alt="image" src="https://github.com/user-attachments/assets/411bbe45-bd81-4d36-9a6e-436be2f73489" />

> gemma-2b-hanyang-Q4_K_M.gguf

<img width="972" height="235" alt="image" src="https://github.com/user-attachments/assets/49af578d-1bc0-42ae-9891-fabf87ea302e" />

> gemma-2b-hanyang-Q4_0.gguf

<img width="1188" height="256" alt="image" src="https://github.com/user-attachments/assets/8373a25e-ebe5-4c6b-b1c1-3fe72508822e" />

> gemma-2b-hanyang-Q4_K_s.gguf

<img width="777" height="246" alt="image" src="https://github.com/user-attachments/assets/913ef302-6df7-4352-9d82-7bb2c97b83ae" />

### ë³´ë“œ íƒ‘ì¬
&nbsp; NVIDIA Jetson orin nano(8GB) ë³´ë“œëŠ” 1,024ê°œì˜ CUDA  Core, 32ê°œì˜ Tensor Coreë¥¼ ê°€ì§„ AI ì¶”ë¡  ë° í•™ìŠµì— íŠ¹í™”ëœ ë³´ë“œë¡œ ì €ì „ë ¥ ë° ê³ ì†ìœ¼ë¡œ ëª¨ë¸ì„ Localë¡œ ë™ì‘ì‹œí‚¤ëŠ”ë° íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ 8GB RAMì„ ê°€ì§€ê³  ìˆì§€ë§Œ, CPUì™€ GPUê°€ í•˜ë‚˜ì˜ 8GB RAMì„ ê³µìœ í•˜ê¸° ë•Œë¬¸ì— RAMìš©ëŸ‰ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ê°€ëŠ¥í•œ ì´ RAM ë©”ëª¨ë¦¬ ìš©ëŸ‰ì€ 6.5GBë¡œ 6.5GBë‚´ì—ì„œ Whisper, VAD, Gemma, TTSëª¨ë¸ì´ ëª¨ë‘ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ì„¤ê³„í•˜ì˜€ìœ¼ë©°, ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. 

- ë³´ë“œ êµ¬ì„±
  
<img width="489" height="322" alt="image" src="https://github.com/user-attachments/assets/55fc32dd-f4cb-4702-815e-46aa62606e59" />

- ë³´ë“œ ë‚´ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ìš©ëŸ‰
  
<img width="995" height="130" alt="image" src="https://github.com/user-attachments/assets/f8fb59fa-d662-403c-a79b-eaa272f7a6b1" />

### ê²°ê³¼
&nbsp; ì½”ë“œë¥¼ êµ¬ì„±í•œ í›„ ëª¨ë¸ì„ ì‘ë™ì‹œí‚¨ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ëª¨ë¸ì€ Ubuntuê¸°ë°˜ ë…¸íŠ¸ë¶ì—ì„œ SSHí†µì‹ ì„ í†µí•´ ì›ê²©ìœ¼ë¡œ ë³´ë“œ ë‚´ì˜ ì½”ë“œë¥¼ ì‘ë™ì‹œì¼°ìŠµë‹ˆë‹¤. ì‚¬ìš©í•œ TTS ëª¨ë¸ì´ ì˜ì–´ base ëª¨ë¸ì´ë©° ê¸°ê³„ìŒì˜ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì¶œë ¥ì´ ë°œìƒí•˜ì—¬ ì¶”í›„ì— ëª¨ë¸ ì„ ì •ì„ í†µí•´ êµì²´í•  ìƒê°ì…ë‹ˆë‹¤.

- ë³´ë“œ ë‚´ ëª¨ë¸ ë¡œë“œ
    
<img width="686" height="516" alt="image" src="https://github.com/user-attachments/assets/fa20084f-f018-404e-a843-9778da14a375" />

- ì‹¤í–‰ ê²°ê³¼
  
<img width="621" height="353" alt="image" src="https://github.com/user-attachments/assets/e8732607-5036-4864-bd84-a9e4ea2a697f" />

- ì‹¤í–‰ ì˜ìƒ
  
https://github.com/user-attachments/assets/cf74bdcc-a74e-4a7b-aecc-20a81747ff87


### ë§ˆë¬´ë¦¬
&nbsp; ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ìƒìš©í™”ê°€ ê°€ëŠ¥í•œ E2Eëª¨ë¸ì€ ì•„ë‹ˆì—ˆì§€ë§Œ, í•œì–‘ëŒ€ ìŒì„± ê¸¸ì•ˆë‚´ ì‹œìŠ¤í…œì„ êµ¬ì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì¶”í›„ì— LLMëª¨ë¸ê³¼ TTSëª¨ë¸ì˜ ê°œì„ ì„ í†µí•´ ì‹¤ì œë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•œì–‘ëŒ€ Local ìŒì„± ì±—ë´‡ì„ ë§Œë“œëŠ” ê²ƒì´ í–¥í›„ ëª©í‘œì…ë‹ˆë‹¤.







