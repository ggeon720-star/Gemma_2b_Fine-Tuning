------
HY-ê±´ë¬¼ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ êµë‚´ ê¸¸ì•ˆë‚´ LLM ë§Œë“¤ê¸°

ê³¼ì • ê°„ë‹¨íˆ
-> APIë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ì…‹ ì •ë³´ ìƒì„±
rawë°ì´í„°ë“¤ì€ ì „ë¶€ APIë¥¼ ì‚¬ìš©í•´ ì–»ì€ ê¸°ë³¸ ê±´ë¬¼ ì •ë³´ë“¤
ì´ë¥¼ gpt1, 2, 3.py ì½”ë“œì™€ clean.py í†µí•´ train_messageì™€ val_messageë¡œ QAë°ì´í„°ì…‹ì„ ìƒì„±í•¨
-> ìƒì„±í•œ ë°ì´í„°ì…‹ì„ train.pyë¡œ í•™ìŠµì‹œì¼œ ëª¨ë¸ ìƒì„±
-> ì´í›„ merge.pyë¥¼ í†µí•´ ëª¨ë¸ì„ ê¸°ì¡´ ëª¨ë¸ê³¼ ë³‘í•©
-> main_merged.pyë¥¼ í†µí•´ì„œ ì‹¤í–‰











# QLoRAë¥¼ ì´ìš©í•œ Gemma-2Bì˜ ë²•ë¥  íŠ¹í™” íŒŒì¸íŠœë‹ 
AIX ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸

# Members
- ê³ ì¬ìœ¤, ìœµí•©ì „ìê³µí•™ë¶€, jaeyun2448@naver.com
- ê¶Œì„±ê·¼, ì›ìë ¥ê³µí•™ê³¼, gbdlzlemr02@gmail.com
- ì‹ ì¤€í¬, ê¸°ê³„ê³µí•™ë¶€, shinjh0331@naver.com
- í•œì¸ê¶Œ, ê¸°ê³„ê³µí•™ë¶€, humanaeiura1023@gmail.com
  
# Index
1. Proposal
2. Datasets
3. Methodology
4. Evaluation & Analysis
5. Related Work
6. Conclusion: Discussion
  
# Proposal
Motivation (Why are you doing this?) :  

&nbsp; í•´ì™¸ì—ì„œëŠ” LLM ê¸°ë°˜ ë²•ë¥  ì„œë¹„ìŠ¤ì˜ ìƒìš©í™”ê°€ ë¹ ë¥´ê²Œ í™•ì‚°ë˜ê³  ìˆì§€ë§Œ, êµ­ë‚´ì—ì„œëŠ” 'ë°ì´í„° ì ‘ê·¼ì„± ë¶€ì¡±, ê°œì¸ì •ë³´ë³´í˜¸ë²•(PIPA)ê³¼ ê°™ì€ ê·œì œ ì¥ë²½, ë²•ì¡°ê³„ì˜ ë³´ìˆ˜ì  íŠ¹ì„±' ë“±ì˜ ì´ìœ ë¡œ ë”ë””ê²Œ í™•ì‚°ë˜ê³  ìˆìŠµë‹ˆë‹¤.
ã€Œê°•ë´‰ì¤€ ì™¸ 1ëª…, êµ­ë‚´ ë²•ë¥  LLMì˜ í™œìš©ê³¼ ì—°êµ¬ë™í–¥ : í™˜ê°ê³¼ ë³´ì•ˆ ë¦¬ìŠ¤í¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œã€

&nbsp; íŠ¹íˆ êµ­ë‚´ ë²•ë¥  AI ë„ì… ê³¼ì •ì—ì„œ í™˜ê° ë° ë³´ì•ˆ ë¦¬ìŠ¤í¬ê°€ ë‹¨ìˆœí•œ ê¸°ìˆ ì  ê²°í•¨ì„ ë„˜ì–´ ì‚¬íšŒì  ë¬¸ì œë¡œ ì—°ê²°ë  ìˆ˜ ìˆìŒìœ¼ë¡œ ì •í™•ë„ ì´ìŠˆë¥¼ ìµœì†Œí™”í•´ì•¼ í•©ë‹ˆë‹¤.
 ê·¸ë ‡ê¸°ì— ì €í¬ëŠ” ê¸°ì¡´ì˜ SLM ëª¨ë¸ (Gemma-2B)ë¥¼ QLoRAë¥¼ í™œìš©í•˜ì—¬ ì €ë¹„ìš©ìœ¼ë¡œ íŒŒì¸íŠœë‹í•¨ìœ¼ë¡œì„œ ë” ì „ë¬¸ì ì´ê³  ë¬¸ë§¥ì„ ì˜ ì´í•´í•˜ëŠ” LLMì„ ë§Œë“¤ê³ ì í•˜ì…¨ìŠµë‹ˆë‹¤.

What do you want to see at the end? : 

1. ë²•ë¥  Domainì—ì„œì˜ ì„±ëŠ¥ í–¥ìƒ
    - íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•œ í‰ê°€ ê¸°ì¤€ í•„ìš”
    - ê¸°ì¡´ Gemma-2Bê³¼ì˜ QA ì •í™•ë„ ë¹„êµ
2. QLoRA (Quantized Low-Rank Adaptation)

# Datasets

# Methodology 
ëŒ€ëµì ì¸ ì•Œê³ ë¦¬ì¦˜
> 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
> 2. Google Drive ë§ˆìš´íŠ¸
> 3. QLoRA í•™ìŠµ ë° LoRA ì–´ëŒ‘í„° ì €ì¥
> 4. í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ Driveì— ë°±ì—… 
> 5. ë² ì´ìŠ¤ ëª¨ë¸ ë° LoRA ì–´ëŒ‘í„°ë¡œ Merged ëª¨ë¸ ë³‘í•©
> 6. í…ŒìŠ¤íŠ¸

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```python
!pip install -q transformers accelerate bitsandbytes peft trl datasets huggingface_hub ipywidgets
```

### 2. Google Drive ë§ˆìš´íŠ¸
```python
from google.colab import drive
drive.mount('/content/drive')
```

### 3. QLoRA í•™ìŠµ ë° LoRA ì–´ëŒ‘í„° ì €ì¥
#### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ë¡œê·¸ì¸
```python
import os
import json
import random
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    EarlyStoppingCallback,
)
from peft import LoraConfig
from datasets import Dataset
from trl import SFTTrainer
from huggingface_hub import login

# Hugging Face ì•¡ì„¸ìŠ¤ í† í° (ì‹¤ì œ ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ ë“±ìœ¼ë¡œ ê´€ë¦¬ ê¶Œì¥)
HF_TOKEN = "<YOUR_HF_TOKEN>"

try:
    login(token=HF_TOKEN)
    print("âœ… HuggingFace ë¡œê·¸ì¸ ì„±ê³µ\n")
except Exception as e:
    print(f"âš ï¸  ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}\n")
```

#### 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ë¡œê·¸ì¸
```python
print("=" * 70)
print("ğŸ“ í•œì–‘ëŒ€í•™êµ ê¸¸ì•ˆë‚´ AI í•™ìŠµ (Colab + QLoRA)")
print("=" * 70)

BASE_DIR = "/content/drive/MyDrive/Gemma_2b_Fine-Tuning"
DATASET_DIR = BASE_DIR

QA_TRAIN_FILES = [
    os.path.join(DATASET_DIR, "train_data_1km_messages.json"),
    os.path.join(DATASET_DIR, "train_data_2km_messages.json"),
    os.path.join(DATASET_DIR, "train_data_in_messages.json"),
]

QA_VAL_FILES = [
    os.path.join(DATASET_DIR, "val_data_1km_messages.json"),
    os.path.join(DATASET_DIR, "val_data_2km_messages.json"),
    os.path.join(DATASET_DIR, "val_data_in_messages.json"),
]

MODEL_ID = "nlpai-lab/ko-gemma-2b-v1"
OUTPUT_DIR = "/content/output/gemma-2b-hanyang-guide-final"
ADAPTER_PATH = "/content/output/gemma-2b-hanyang-guide-lora-final"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(ADAPTER_PATH, exist_ok=True)

print(f"ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸: {MODEL_ID}")
print(f"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}")
print(f"ğŸ“ ë°ì´í„° í´ë”: {DATASET_DIR}")
print("=" * 70 + "\n")
```

#### 3. GPU í™•ì¸
```python
print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸")
if torch.cuda.is_available():
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    USE_GPU = True
else:
    print("âš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Colabì—ì„œ GPU ëŸ°íƒ€ì„ì„ ì„¤ì •í•˜ì„¸ìš”.")
    USE_GPU = False
print()
```

#### 4. QLoRA ë° LORA ì„¤ì •
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)

print("=" * 70)
print("ğŸ“‹ í•™ìŠµ ì„¤ì • (QLoRA + LoRA)")
print("=" * 70)
print("ëª¨ë¸ í¬ê¸°: 2B parameters")
print("LoRA rank: 16")
print("LoRA alpha: 32")
print("=" * 70 + "\n")
```

#### 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
```python
print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘... ({MODEL_ID})")

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    local_files_only=False,
)
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,
    local_files_only=False,
)

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"ğŸ“ Chat template ì¡´ì¬: {tokenizer.chat_template is not None}")
print(f"ğŸ”¢ Vocab size: {tokenizer.vocab_size:,}")
print()
```

#### 6. ë°ì´í„°ì…‹ ë¡œë“œ (message í¬ë§·)
```python
print("=" * 70)
print("ğŸ“‚ ë°ì´í„° ë¡œë“œ (messages í¬ë§·)")
print("=" * 70)

def load_messages_data(file_paths, dataset_type="Train"):
    """messages í˜•ì‹ json íŒŒì¼ì„ ë¡œë“œí•˜ê³  chat templateë¡œ í•˜ë‚˜ì˜ textë¡œ ë³€í™˜"""
    all_texts = []

    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"âš ï¸  {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            if isinstance(data, list):
                for item in data:
                    if "messages" in item and isinstance(item["messages"], list):
                        try:
                            text = tokenizer.apply_chat_template(
                                item["messages"],
                                tokenize=False,
                                add_generation_prompt=False,
                            )
                            all_texts.append(text)
                        except Exception as e:
                            print(f"âš ï¸  Chat template ì ìš© ì‹¤íŒ¨: {e}")
                            print(f"   Messages: {item['messages']}")
                    else:
                        print(f"âš ï¸  ì˜ëª»ëœ í¬ë§·: {item}")

                print(f"âœ… {os.path.basename(file_path)}: {len(data)}ê°œ ë¡œë“œ")
            else:
                print(f"âš ï¸  {file_path} í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (list ì•„ë‹˜).")

        except Exception as e:
            print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")

    print(f"\nğŸ“Š ì´ {dataset_type} ë°ì´í„°: {len(all_texts)}ê°œ")
    return all_texts

print("\n[Train ë°ì´í„°]")
train_texts = load_messages_data(QA_TRAIN_FILES, "Train")

print("\n[Validation ë°ì´í„°]")
val_texts = load_messages_data(QA_VAL_FILES, "Validation")

# Validation ë°ì´í„°ê°€ ì—†ìœ¼ë©´ Trainì—ì„œ 10%ë¥¼ ë¶„ë¦¬
if not val_texts and train_texts:
    print("âš ï¸  Validation ë°ì´í„°ê°€ ì—†ì–´ Trainì—ì„œ 10%ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.")
    split_idx = int(len(train_texts) * 0.9)
    val_texts = train_texts[split_idx:]
    train_texts = train_texts[:split_idx]

train_dataset = Dataset.from_dict({"text": train_texts}) if train_texts else Dataset.from_dict({"text": []})
eval_dataset = Dataset.from_dict({"text": val_texts}) if val_texts else Dataset.from_dict({"text": []})

print("\n" + "=" * 70)
print("ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°")
print("=" * 70)
print(f"Train: {len(train_dataset):,}ê°œ")
print(f"Eval:  {len(eval_dataset):,}ê°œ")
print(f"Total: {len(train_dataset) + len(eval_dataset):,}ê°œ")
print("=" * 70 + "\n")

if len(train_dataset) > 0:
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„°:")
    print("-" * 70)
    sample_text = train_dataset[0]["text"]
    print("í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì):")
    print(sample_text[:500])
    print("...")
    print("-" * 70 + "\n")
else:
    print("âš ï¸  Train ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤. ê²½ë¡œì™€ json êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n")
```

#### 7. formatting_func ì •ì˜
```python
def formatting_func(example):
    """text í•„ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©"""
    return example["text"]
```



#### 8. SFTTrainer ì„¤ì •
```python
print("âš™ï¸  Trainer ì„¤ì • ì¤‘...\n")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,

    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,

    gradient_checkpointing=True,
    max_grad_norm=1.0,

    optim="paged_adamw_8bit",

    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.01,

    eval_strategy="steps",
    eval_steps=100,
    save_steps=100,
    save_total_limit=3,

    fp16=True,
    bf16=False,

    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=10,
    report_to="tensorboard",
)

print("=" * 70)
print("ğŸ“‹ ìµœì¢… í•™ìŠµ ì„¤ì • ìš”ì•½")
print("=" * 70)
effective_batch = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps
print(f"ì‹¤ì§ˆ ë°°ì¹˜ í¬ê¸°: {effective_batch}")
if len(train_dataset) > 0:
    total_steps = len(train_dataset) * training_args.num_train_epochs // effective_batch
else:
    total_steps = 0
print(f"ì˜ˆìƒ ìŠ¤í… ìˆ˜: {total_steps:,}")
print(f"í•™ìŠµë¥ : {training_args.learning_rate}")
print("=" * 70 + "\n")

early_stopping = EarlyStoppingCallback(early_stopping_patience=3)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    formatting_func=formatting_func,
    callbacks=[early_stopping],
)
```




#### 9. Training
```python
print("=" * 70)
print("ğŸš€ í•™ìŠµ ì‹œì‘")
print("=" * 70)
print("ğŸ’¡ êµ¬ì„± ìš”ì•½:")
print("   - messages í¬ë§· json 6ê°œ ì‚¬ìš©")
print("   - tokenizer.apply_chat_template()ë¡œ text ìƒì„±")
print("   - QLoRA (4bit) + LoRA")
print("=" * 70 + "\n")

if len(train_dataset) == 0:
    print("âš ï¸  Train ë°ì´í„°ê°€ 0ê°œë¼ í•™ìŠµì„ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
else:
    try:
        trainer.train()

        print("\n" + "=" * 70)
        print("âœ… í•™ìŠµ ì™„ë£Œ")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\nâš ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

#%% ==========================
# 9. LoRA ì–´ëŒ‘í„° ì €ì¥
#==============================
print("\n" + "=" * 70)
print("ğŸ’¾ LoRA ì–´ëŒ‘í„° ì €ì¥")
print("=" * 70)

try:
    trainer.model.save_pretrained(ADAPTER_PATH)
    tokenizer.save_pretrained(ADAPTER_PATH)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {ADAPTER_PATH}")

    print("\n" + "=" * 70)
    print("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    print("=" * 70)
    print(f"ğŸ“ LoRA ì–´ëŒ‘í„° ê²½ë¡œ: {ADAPTER_PATH}")
    print("\nâš ï¸  ì¶”ë¡  ì‹œì—ë„ tokenizer.apply_chat_template()ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.")
    print("=" * 70)

except Exception as e:
    print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

print("\nâœ… ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ")
print("=" * 70)
```


### 4. í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ Driveì— ë°±ì—…
```python
!mkdir -p /content/drive/MyDrive/Gemma_2B_Trained
!cp -r /content/output/gemma-2b-hanyang-guide-lora-final /content/drive/MyDrive/Gemma_2B_Trained/
```


### 5. ë² ì´ìŠ¤ ëª¨ë¸ ë° LoRA ì–´ëŒ‘í„° Merged ëª¨ë¸ ë³‘í•©
```python
os.makedirs(OUTPUT_DIR, exist_ok=True)
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,                     # Epoch
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    eval_strategy="steps",
    eval_steps=0.2,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=10,
    warmup_steps=5,
    logging_strategy="steps",
    learning_rate=2e-4,                     # í•™ìŠµë¥ 
    fp16=True,
    report_to="tensorboard",
    save_strategy="epoch",
    load_best_model_at_end=False,
)
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    formatting_func=lambda x: x["text"],
)
```

### 6. í•™ìŠµ ì‹¤í–‰ ë° LoRA ì–´ëŒ‘í„° ì €ì¥, ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©
```python
trainer.train()
os.makedirs(ADAPTER_PATH, exist_ok=True)
trainer.model.save_pretrained(ADAPTER_PATH)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map='auto',
    torch_dtype=torch.bfloat16
)

model = PeftModel.from_pretrained(base_model, ADAPTER_PATH, device_map='auto', torch_dtype=torch.bfloat16)
model = model.merge_and_unload()

os.makedirs(MERGED_PATH, exist_ok=True)
model.save_pretrained(MERGED_PATH)
tokenizer.save_pretrained(MERGED_PATH)
```

# LLMì˜ ì„±ëŠ¥ í‰ê°€ ê¸°ì¤€/ë°©ì‹
## 1. Intrinsic / Extrinsic Evaluation : ëª¨ë¸ì´ ì–¸ì–´ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‰ê°€
### - perplextiy
#### <img width="172" height="42" alt="image" src="https://github.com/user-attachments/assets/6155d5ed-3fab-4560-9378-0f369d9841b3" />
#### Nì€ ì´ í† í° ìˆ˜ p(x_i)ëŠ” x_ië²ˆì§¸ ì •ë‹µ í† í°ì„ ì´ ëª¨ë¸ì´ ë§ì¶œ í™•ë¥ 
#### í‰ê· ì ìœ¼ë¡œ ëª¨ë¸ì´ ì •ë‹µ í† í°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ë‚®ì€ í˜¼ë€ë„ë¥¼ ê°–ëŠ”ì§€ ê³„ì‚°í•¨

## 2. Task-based Evaluation ëª¨ë¸ì˜ ì‹¤ì œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ
### - MMLU
#### 57ê°œ ë¶„ì•¼ì˜ ì‹œí—˜ ë¬¸ì œì˜ ì •í™•ë„ë¥¼ í‰ê°€
#### ì‚¬ëŒ / GPT-4ë“±ì˜ ìˆ˜ì¤€ ë¹„êµì— ì‚¬ìš©

### - GSM8K
#### Grade-school math ë¬¸ì œ í’€ì´ ì •í™•ë„ë¥¼ í‰ê°€
#### LLMì˜ ìˆ˜í•™ì  ì¶”ë¡ ì„ ì§ì ‘ì ìœ¼ë¡œ í‰ê°€í•¨

### - ARC / HellaSwag / WinoGrande ë“±...

## 3. Safty / Alignment Evaluation
### - Hallucination Rate
#### ëª¨ë¸ì˜ ì¶œë ¥ì´ ì‚¬ì‹¤ê³¼ ë§ì§€ ì•Šì„ ë•Œì˜ ë¹„ìœ¨
#### ì˜¤ë¥˜ ì‘ë‹µ ìˆ˜ / ì „ì²´ ì‘ë‹µ ìˆ˜

## 4. Text Genration Quality
### - BLEU
#### n-gram precision ê¸°ë°˜
#### <img width="172" height="42" alt="518252181-4e280d64-cc25-4b3b-a31b-2fcd956d9266" src="https://github.com/user-attachments/assets/b5694356-043b-4605-8d55-d306905199de" />
#### p_nì€ ì˜ˆì¸¡ ë¬¸ì¥ê³¼ ì°¸ì¡° ë¬¸ì¥ì—ì„œ ì¼ì¹˜í•œ ìˆ˜ / ì˜ˆì¸¡ë¬¸ì¥ì˜ ì „ì²´ n-gram ìˆ˜ / w_nì€ ê°€ì¤‘ì¹˜

## 5. System-level Evaluation

# Related Work (e.g., existing studies)
#### Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Supryadi, Yu, L., Liu, Y., Li, J., Xiong, B., & Xiong, D. (2023, November 25). Evaluating large language models: A comprehensive survey (arXiv pre-print arXiv:2310.19736).
#### ê°•ë´‰ì¤€, & ê¹€ì˜ì¤€. (2025). êµ­ë‚´ ë²•ë¥  LLMì˜ í™œìš©ê³¼ ì—°êµ¬ë™í–¥ : í™˜ê°ê³¼ ë³´ì•ˆ ë¦¬ìŠ¤í¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ. ì‚°ì—…ê¸°ìˆ ì—°êµ¬ë…¼ë¬¸ì§€, 30(3), 227-240.
# Conclusion : Discussion
