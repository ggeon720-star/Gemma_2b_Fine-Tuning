import json, os
import re
from openai import OpenAI
from tqdm import tqdm
from time import sleep
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("âŒ OPENAI_API_KEY not found in .env file. Please check your .env file.")

model_name = "gpt-4o-mini"

client = OpenAI(api_key=api_key)

print(f"âœ“ API key loaded successfully")
print(f"âœ“ Using model: {model_name}")
print(f"âœ“ Generating bilingual QA pairs for MID-DISTANCE locations (1-2km, Public Transport)")
print(f"  Focus: Bus/Subway routes, stop IDs, transfer info\n")


def load_input_json(json_path):
    """ì§€ì •ëœ ê²½ë¡œì˜ JSON íŒŒì¼ì„ ì½ì–´ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"Loaded {len(data)} mid-distance locations from {json_path}")
    return data


def generate_qa_batch(location_info, batch_type, batch_num, language="korean", max_retries=3):
    """
    1-2km ê±°ë¦¬ ì¥ì†Œì˜ ëŒ€ì¤‘êµí†µ ê²½ë¡œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    batch_type: 'basic', 'transit', 'route_detail', 'complex' ì¤‘ í•˜ë‚˜
    language: 'korean' ë˜ëŠ” 'english'
    """
    location_str = json.dumps(location_info, ensure_ascii=False, indent=2)
    
    # ì–¸ì–´ë³„ ì„¤ì •
    if language == "korean":
        lang_instruction = "í•œêµ­ì–´ë¡œ"
        lang_note = "ë°˜ë§ ìœ„ì£¼, ì¡´ëŒ“ë§ ì¼ë¶€ í˜¼ìš©"
        example_questions = {
            'basic': '"ë¬´í•™ì¤‘í•™êµ ì–´ë”” ìˆì–´?", "ë¬´í•™ì¤‘í•™êµëŠ” ë­ì•¼?", "ë¬´í•™ì¤‘í•™êµ ì–¼ë§ˆë‚˜ ë©€ì–´?"',
            'transit': '"ë¬´í•™ì¤‘í•™êµ ë²„ìŠ¤ íƒ€ê³  ê°€ì•¼ í•´?", "ë¬´í•™ì¤‘í•™êµ ê°€ëŠ” ë²„ìŠ¤ ë­ì•¼?", "ë¬´í•™ì¤‘í•™êµ ì •ë¥˜ì¥ ì–´ë””ì•¼?"',
            'route_detail': '"ë¬´í•™ì¤‘í•™êµ ê°€ëŠ” ë°©ë²• ìì„¸íˆ ì•Œë ¤ì¤˜", "ì–´ëŠ ì¶œêµ¬ë¡œ ë‚˜ê°€ì•¼ í•´?", "ì–´ë””ì„œ ë‚´ë ¤ì•¼ í•´?"',
            'complex': '"ë¬´í•™ì¤‘í•™êµ ê°€ëŠ”ë° ì‹œê°„ ì–¼ë§ˆë‚˜ ê±¸ë ¤?", "ë¬´í•™ì¤‘í•™êµê¹Œì§€ ê°€ì¥ ë¹ ë¥¸ ë°©ë²•ì€?", "ê±¸ì–´ê°ˆ ìˆ˜ë„ ìˆì–´?"'
        }
    else:  # english
        lang_instruction = "ì˜ì–´ë¡œ"
        lang_note = "ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ ì˜ì–´ (informal/conversational)"
        example_questions = {
            'basic': '"Where is Muhak Middle School?", "What is Muhak Middle School?", "How far is it?"',
            'transit': '"Do I need to take a bus?", "Which bus goes there?", "Where is the bus stop?"',
            'route_detail': '"Can you explain the route in detail?", "Which exit should I use?", "Where should I get off?"',
            'complex': '"How long does it take?", "What\'s the fastest way?", "Can I walk there?"'
        }
    
    # ë°°ì¹˜ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ëŒ€ì¤‘êµí†µ ì¤‘ì‹¬)
    prompts = {
        'basic': {
            'count': 8,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 8ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ì¥ì†Œ ì´ë¦„/ìœ„ì¹˜ ì§ˆë¬¸ (3ê°œ)
            - ì¥ì†Œ íŠ¹ì§•/ì¹´í…Œê³ ë¦¬ ì§ˆë¬¸ (2ê°œ)
            - ê±°ë¦¬/ì ‘ê·¼ì„± ì§ˆë¬¸ (3ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['basic']}
            """
        },
        'transit': {
            'count': 12,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 12ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ëŒ€ì¤‘êµí†µ ì´ìš© í•„ìš”ì„± ì§ˆë¬¸ (3ê°œ)
            - ë²„ìŠ¤ ë²ˆí˜¸/ë…¸ì„  ì§ˆë¬¸ (4ê°œ)
            - ì •ë¥˜ì¥ ìœ„ì¹˜ ì§ˆë¬¸ (3ê°œ)
            - ì¶œêµ¬/í™˜ìŠ¹ ì •ë³´ ì§ˆë¬¸ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['transit']}
            """
        },
        'route_detail': {
            'count': 12,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 12ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ìƒì„¸ ê²½ë¡œ ì•ˆë‚´ ì§ˆë¬¸ (5ê°œ)
            - í•˜ì°¨ ìœ„ì¹˜ ì§ˆë¬¸ (3ê°œ)
            - ë„ë³´ ì‹œê°„ vs ë²„ìŠ¤ ì‹œê°„ ë¹„êµ (2ê°œ)
            - ì¶œêµ¬/íƒ‘ìŠ¹ ìœ„ì¹˜ ì§ˆë¬¸ (2ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['route_detail']}
            """
        },
        'complex': {
            'count': 8,
            'instruction': f"""
            ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ {lang_instruction} 8ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:
            - ë³µí•© ì •ë³´ ì§ˆë¬¸ (ì¥ì†Œ + ëŒ€ì¤‘êµí†µ ê²½ë¡œ) (3ê°œ)
            - ìµœì  ê²½ë¡œ/ì‹œê°„ ì§ˆë¬¸ (2ê°œ)
            - ë„ë³´ vs ëŒ€ì¤‘êµí†µ ë¹„êµ (2ê°œ)
            - ëŒ€ì²´ ê²½ë¡œ ì§ˆë¬¸ (1ê°œ)
            
            ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ: {example_questions['complex']}
            """
        }
    }
    
    batch_config = prompts[batch_type]
    
    if language == "korean":
        prompt = f"""
ë‹¹ì‹ ì€ í•œì–‘ëŒ€í•™êµ ì£¼ë³€ ì§€ì—­ ëŒ€ì¤‘êµí†µ ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì¥ì†Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒë“¤ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸-ë‹µë³€ ìŒì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.

[ì¥ì†Œ ì •ë³´ - 1~2km ê±°ë¦¬, ëŒ€ì¤‘êµí†µ ê¶Œì¥]
```json
{location_str}
```

[ìƒì„± ì§€ì¹¨]
{batch_config['instruction']}

[ì¤‘ìš” ê·œì¹™]
1. ì§ˆë¬¸ì€ êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„± ({lang_note})
2. ì§ˆë¬¸ í‘œí˜„ì„ ìµœëŒ€í•œ ë‹¤ì–‘í•˜ê²Œ:
   - ì¥ì†Œëª… ë³€í˜•: "{location_info.get('name', '')}", ì•½ì¹­ì´ë‚˜ ë³„ì¹­ë„ í™œìš©
   - ì§ˆë¬¸ í˜•ì‹: "~ì–´ë””ì•¼?", "~ì•Œë ¤ì¤˜", "~ì–´ë–»ê²Œ ê°€?", "~ë²„ìŠ¤ ë­ íƒ€?", "~ì •ë¥˜ì¥ ì–´ë””?" ë“±
3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ, ì£¼ì–´ì§„ ì •ë³´ì—ë§Œ ê·¼ê±°
4. ë‹µë³€ ê¸¸ì´: 200-350ì ë‚´ì™¸ (ëŒ€ì¤‘êµí†µ ê²½ë¡œ ì„¤ëª…)
5. ê° QAëŠ” ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ë‚´ìš©ì´ì–´ì•¼ í•¨ (ì¤‘ë³µ ìµœì†Œí™”)
6. ëŒ€ì¤‘êµí†µ ì •ë³´ë¥¼ ì •í™•íˆ í¬í•¨:
   - ì§€í•˜ì²  ì¶œêµ¬ ë²ˆí˜¸
   - ë²„ìŠ¤ ë²ˆí˜¸ (ì˜ˆ: ì„±ë™03, 2012ë²ˆ)
   - ì •ë¥˜ì¥ ì´ë¦„ (ì˜ˆ: 'í•œì–‘ëŒ€ì—­' ì •ë¥˜ì¥)
   - í•˜ì°¨ ì •ë¥˜ì¥
   âš ï¸ ì •ë¥˜ì¥ IDëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: ID: 04-129 ê°™ì€ ì •ë³´ ì œì™¸)
7. estimated_time_walking ì •ë³´ë¥¼ í™œìš© (ë„ë³´ ì‹œê°„ vs ë²„ìŠ¤ ì‹œê°„ ë¹„êµ)
8. transport_typeì´ "ëŒ€ì¤‘êµí†µ ê¶Œì¥"ì„ì„ ë°˜ì˜
9. 1~2km ê±°ë¦¬ì´ë¯€ë¡œ "ê±¸ì–´ê°€ê¸°ì—” ì¢€ ë©€ì–´ì„œ ë²„ìŠ¤ ì¶”ì²œ" ë‰˜ì•™ìŠ¤ í¬í•¨

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:

```json
[
  {{
    "question": "ìƒì„±ëœ ì§ˆë¬¸ 1",
    "answer": "ìƒì„±ëœ ë‹µë³€ 1",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }},
  {{
    "question": "ìƒì„±ëœ ì§ˆë¬¸ 2",
    "answer": "ìƒì„±ëœ ë‹µë³€ 2",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }}
]
```

ì •í™•íˆ {batch_config['count']}ê°œì˜ QA ìŒì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
    else:  # english
        prompt = f"""
You are a public transportation guide expert for the Hanyang University area.
Based on the location information below, generate natural question-answer pairs in English that students would actually ask.

[Location Information - 1-2km distance, Public Transport Recommended]
```json
{location_str}
```

[Generation Guidelines]
{batch_config['instruction']}

[Important Rules]
1. Questions should be written in natural conversational style ({lang_note})
2. Vary question expressions as much as possible:
   - Location name variations: "{location_info.get('name', '')}", use abbreviations or common names
   - Question formats: "Where is~?", "Which bus~?", "How do I get~?", "Where's the bus stop~?", "What's the stop ID~?" etc.
3. Answers should be clear and friendly, based only on the given information
4. Answer length: 200-350 characters for public transport route descriptions
5. Each QA should be clearly distinct (minimize duplication)
6. Include accurate public transport information:
   - Subway exit numbers
   - Bus numbers (e.g., Seongdong 03, Bus 2012)
   - Bus stop names (e.g., 'Hanyang Univ. Station' stop)
   - Drop-off stop
   âš ï¸ Do NOT include stop IDs (e.g., exclude information like ID: 04-129)
7. Use estimated_time_walking information (compare walking vs bus time)
8. Reflect that transport_type is "Public Transport Recommended"
9. Distance is 1-2km, so mention "a bit far to walk, bus recommended" nuance

[Output Format]
Output ONLY in the following JSON array format:

```json
[
  {{
    "question": "Generated question 1",
    "answer": "Generated answer 1",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }},
  {{
    "question": "Generated question 2",
    "answer": "Generated answer 2",
    "type": "{batch_type}",
    "location_name": "{location_info.get('name', '')}"
  }}
]
```

Generate exactly {batch_config['count']} QA pairs.
"""

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=4000
            )
            content = response.choices[0].message.content.strip()
            
            if not content:
                print(f"  âš ï¸ Empty response for {language} {batch_type} batch {batch_num} (Attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    sleep(2)
                    continue
                return []
            
            # JSON íŒŒì‹±
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', content, re.DOTALL)
            if not json_match:
                json_match = re.search(r'(\[.*?\])', content, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1)
                parsed_list = json.loads(json_str)
                
                if isinstance(parsed_list, list) and len(parsed_list) > 0:
                    print(f"  âœ“ Generated {len(parsed_list)} {language} QAs for {batch_type} batch {batch_num}")
                    return parsed_list
                else:
                    raise ValueError("Parsed JSON is empty or invalid")
            else:
                raise ValueError("No JSON array found in response")
        
        except (json.JSONDecodeError, ValueError) as e:
            print(f"  âš ï¸ Parse error for {language} {batch_type} batch {batch_num}: {e} (Attempt {attempt + 1})")
            if attempt < max_retries - 1:
                sleep(2)
        except Exception as e:
            print(f"  âŒ API error for {language} {batch_type} batch {batch_num}: {e} (Attempt {attempt + 1})")
            if attempt < max_retries - 1:
                sleep(3)
    
    return []


def generate_qa_pairs_for_location(location_info, language="korean"):
    """
    ë‹¨ì¼ ì¥ì†Œì— ëŒ€í•´ 40ê°œì˜ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë°°ì¹˜ë³„ë¡œ ë‚˜ëˆ ì„œ ìƒì„±í•˜ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
    """
    location_name = location_info.get('name', 'Unknown')
    lang_label = "ğŸ‡°ğŸ‡·" if language == "korean" else "ğŸ‡ºğŸ‡¸"
    print(f"\nğŸšŒ {lang_label} Processing: {location_name} ({language.upper()})")
    
    all_qa_pairs = []
    
    # ë°°ì¹˜ë³„ ìƒì„± (ì´ 40ê°œ ëª©í‘œ) - ëŒ€ì¤‘êµí†µ ì¤‘ì‹¬
    batches = [
        ('basic', 1),         # 8ê°œ
        ('transit', 1),       # 12ê°œ - ë²„ìŠ¤/ì •ë¥˜ì¥ ì •ë³´
        ('route_detail', 1),  # 12ê°œ - ìƒì„¸ ê²½ë¡œ
        ('complex', 1),       # 8ê°œ
    ]
    
    for batch_type, batch_num in batches:
        qa_batch = generate_qa_batch(location_info, batch_type, batch_num, language)
        all_qa_pairs.extend(qa_batch)
        sleep(1)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
    
    print(f"  ğŸ“Š Total generated: {len(all_qa_pairs)} {language} QA pairs")
    return all_qa_pairs


def generate_all_qa_pairs(input_data):
    """
    ëª¨ë“  ì¥ì†Œì— ëŒ€í•´ í•œêµ­ì–´ì™€ ì˜ì–´ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    korean_qa_pairs = []
    english_qa_pairs = []
    korean_counter = 0
    english_counter = 0
    
    # ì¹´í…Œê³ ë¦¬ í•œì˜ ë§¤í•‘
    category_map = {
        "êµìœ¡": "Education",
        "ìƒì—…ì‹œì„¤": "Commercial",
        "ì˜ë£Œ": "Medical",
        "ë¬¸í™”/ì²´ìœ¡": "Culture/Sports",
        "ê³µê³µì‹œì„¤": "Public Facility",
        "ì£¼ê±°": "Residential",
        "êµí†µ": "Transportation",
        "í–‰ì •/ë¶€ì†ê±´ë¬¼": "Administrative/Auxiliary",
        "í•™ìˆ ": "Academic",
        "ê¸°ìˆ™ì‚¬": "Dormitory",
        "ì²´ìœ¡ì‹œì„¤": "Sports Facility",
        "ê¸°íƒ€": "Others"
    }
    
    for location in tqdm(input_data, desc="Generating bilingual QA dataset (1-2km, Public Transport)"):
        try:
            # í•œêµ­ì–´ QA ìƒì„±
            korean_pairs = generate_qa_pairs_for_location(location, language="korean")
            
            if korean_pairs:
                for qa in korean_pairs:
                    qa_pair = {
                        "id": f"KO_MID_{korean_counter:05d}",
                        "language": "korean",
                        "question": qa.get("question", ""),
                        "answer": qa.get("answer", ""),
                        "type": qa.get("type", "unknown"),
                        "location_name": qa.get("location_name", ""),
                        "category": location.get("category", "")
                    }
                    korean_qa_pairs.append(qa_pair)
                    korean_counter += 1
            else:
                print(f"âš ï¸ Warning: No Korean QA pairs generated for {location.get('name', 'Unknown')}")
            
            sleep(2)  # í•œì˜ ìƒì„± ì‚¬ì´ ëŒ€ê¸°
            
            # ì˜ì–´ QA ìƒì„±
            english_pairs = generate_qa_pairs_for_location(location, language="english")
            
            english_category = category_map.get(location.get("category", ""), location.get("category", ""))
            
            if english_pairs:
                for qa in english_pairs:
                    qa_pair = {
                        "id": f"EN_MID_{english_counter:05d}",
                        "language": "english",
                        "question": qa.get("question", ""),
                        "answer": qa.get("answer", ""),
                        "type": qa.get("type", "unknown"),
                        "location_name": qa.get("location_name", ""),
                        "category": english_category
                    }
                    english_qa_pairs.append(qa_pair)
                    english_counter += 1
            else:
                print(f"âš ï¸ Warning: No English QA pairs generated for {location.get('name', 'Unknown')}")
        
        except Exception as e:
            print(f"âŒ Error processing {location.get('name', 'Unknown')}: {e}")
    
    return korean_qa_pairs, english_qa_pairs


def save_qa_pairs_to_json(qa_pairs, output_path, language):
    """ìƒì„±ëœ QA ìŒ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Saved {language} QA pairs to: {output_path}")


def print_statistics(korean_qa, english_qa):
    """ìƒì„±ëœ QA ë°ì´í„°ì…‹ì˜ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*70)
    print("ğŸ“Š MID-DISTANCE (1-2KM) BILINGUAL DATASET STATISTICS")
    print("="*70)
    
    # í•œêµ­ì–´ í†µê³„
    print("\nğŸ‡°ğŸ‡· KOREAN QA PAIRS")
    print("-" * 70)
    print(f"Total: {len(korean_qa)}")
    
    ko_type_counts = {}
    for qa in korean_qa:
        qa_type = qa.get('type', 'unknown')
        ko_type_counts[qa_type] = ko_type_counts.get(qa_type, 0) + 1
    
    print("\n[By Type]")
    for qa_type, count in sorted(ko_type_counts.items()):
        print(f"  {qa_type:15s}: {count:4d} ({count/len(korean_qa)*100:.1f}%)")
    
    ko_location_counts = {}
    for qa in korean_qa:
        location = qa.get('location_name', 'Unknown')
        ko_location_counts[location] = ko_location_counts.get(location, 0) + 1
    
    print(f"\n[By Location]")
    if len(ko_location_counts) > 0:
        print(f"  Total locations: {len(ko_location_counts)}")
        print(f"  Avg QA per location: {len(korean_qa)/len(ko_location_counts):.1f}")
        print(f"  Min: {min(ko_location_counts.values())}, Max: {max(ko_location_counts.values())}")
    
    # ì˜ì–´ í†µê³„
    print("\n" + "-" * 70)
    print("ğŸ‡ºğŸ‡¸ ENGLISH QA PAIRS")
    print("-" * 70)
    print(f"Total: {len(english_qa)}")
    
    en_type_counts = {}
    for qa in english_qa:
        qa_type = qa.get('type', 'unknown')
        en_type_counts[qa_type] = en_type_counts.get(qa_type, 0) + 1
    
    print("\n[By Type]")
    for qa_type, count in sorted(en_type_counts.items()):
        print(f"  {qa_type:15s}: {count:4d} ({count/len(english_qa)*100:.1f}%)")
    
    en_location_counts = {}
    for qa in english_qa:
        location = qa.get('location_name', 'Unknown')
        en_location_counts[location] = en_location_counts.get(location, 0) + 1
    
    print(f"\n[By Location]")
    if len(en_location_counts) > 0:
        print(f"  Total locations: {len(en_location_counts)}")
        print(f"  Avg QA per location: {len(english_qa)/len(en_location_counts):.1f}")
        print(f"  Min: {min(en_location_counts.values())}, Max: {max(en_location_counts.values())}")
    
    # ì „ì²´ í†µê³„
    print("\n" + "-" * 70)
    print("ğŸŒ COMBINED STATISTICS")
    print("-" * 70)
    print(f"Total QA pairs (Korean + English): {len(korean_qa) + len(english_qa)}")
    print(f"Korean: {len(korean_qa)} ({len(korean_qa)/(len(korean_qa)+len(english_qa))*100:.1f}%)")
    print(f"English: {len(english_qa)} ({len(english_qa)/(len(korean_qa)+len(english_qa))*100:.1f}%)")
    
    print("="*70 + "\n")


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    
    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    input_json_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\hanyang_routes_gpt_1_2km.json"
    korean_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\mid_distance_qa_korean.json"
    english_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\mid_distance_qa_english.json"
    combined_output_path = r"C:\Users\jaeyu\Desktop\gemma\LLM\mid_distance_qa_combined.json"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(korean_output_path), exist_ok=True)
    
    try:
        # 1. ì…ë ¥ JSON íŒŒì¼ ë¡œë“œ
        input_data = load_input_json(input_json_path)
        
        # 2. í•œì˜ QA ìŒ ìƒì„±
        if isinstance(input_data, list):
            korean_qa, english_qa = generate_all_qa_pairs(input_data)
        elif isinstance(input_data, dict):
            print("Input is a single object, wrapping in a list.")
            korean_qa, english_qa = generate_all_qa_pairs([input_data])
        else:
            print("âŒ Error: Input JSON format is not a list or object.")
            korean_qa, english_qa = [], []
        
        if not korean_qa and not english_qa:
            print("âŒ No QA pairs were generated. Please check your input data and API.")
            exit(1)
        
        # 3. ìƒ˜í”Œ ì¶œë ¥
        print("\n" + "="*70)
        print("ğŸ“ SAMPLE QA PAIRS (1-2KM, PUBLIC TRANSPORT)")
        print("="*70)
        
        if korean_qa:
            print("\nğŸ‡°ğŸ‡· Korean Sample (First 2):")
            for i, qa in enumerate(korean_qa[:2], 1):
                print(f"\n[KO Sample {i}]")
                print(f"Q: {qa['question']}")
                print(f"A: {qa['answer'][:150]}..." if len(qa['answer']) > 150 else f"A: {qa['answer']}")
                print(f"Type: {qa['type']}, Location: {qa['location_name']}")
        
        if english_qa:
            print("\nğŸ‡ºğŸ‡¸ English Sample (First 2):")
            for i, qa in enumerate(english_qa[:2], 1):
                print(f"\n[EN Sample {i}]")
                print(f"Q: {qa['question']}")
                print(f"A: {qa['answer'][:150]}..." if len(qa['answer']) > 150 else f"A: {qa['answer']}")
                print(f"Type: {qa['type']}, Location: {qa['location_name']}")
        
        print("="*70)
        
        # 4. í†µê³„ ì¶œë ¥
        print_statistics(korean_qa, english_qa)
        
        # 5. JSON íŒŒì¼ë¡œ ì €ì¥ (ë¶„ë¦¬)
        save_qa_pairs_to_json(korean_qa, korean_output_path, "Korean")
        save_qa_pairs_to_json(english_qa, english_output_path, "English")
        
        # 6. í†µí•© íŒŒì¼ë„ ì €ì¥
        combined_qa = korean_qa + english_qa
        save_qa_pairs_to_json(combined_qa, combined_output_path, "Combined")
        
        print("\n" + "="*70)
        print("âœ… Successfully generated MID-DISTANCE bilingual QA dataset!")
        print("="*70)
        print(f"ğŸ‡°ğŸ‡· Korean QA pairs: {len(korean_qa)}")
        print(f"   Target: {len(input_data)} locations Ã— 40 = {len(input_data) * 40}")
        print(f"   Achievement: {len(korean_qa)/(len(input_data)*40)*100:.1f}%")
        
        print(f"\nğŸ‡ºğŸ‡¸ English QA pairs: {len(english_qa)}")
        print(f"   Target: {len(input_data)} locations Ã— 40 = {len(input_data) * 40}")
        print(f"   Achievement: {len(english_qa)/(len(input_data)*40)*100:.1f}%")
        
        print(f"\nğŸŒ Total QA pairs: {len(korean_qa) + len(english_qa)}")
        print(f"\nğŸšŒ Focus: Bus routes, stop names, exit numbers (NO stop IDs)")
        print("="*70)

    except FileNotFoundError:
        print(f"âŒ Error: Input file not found at {input_json_path}")
    except Exception as e:
        print(f"âŒ An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()