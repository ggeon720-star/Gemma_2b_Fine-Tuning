"""
Ko-Gemma ì¶”ë¡  ì½”ë“œ (Merged ëª¨ë¸ìš©)
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ========================================================================
# 1. ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# ========================================================================

# â­ Merged ëª¨ë¸ì€ ë‹¨ì¼ ê²½ë¡œë§Œ í•„ìš”
MERGED_MODEL_PATH = "./output/gemma-2b-hanyang-final-merged"

print("="*70)
print("ğŸ“ í•œì–‘ëŒ€í•™êµ ê¸¸ì•ˆë‚´ AI - ì¶”ë¡  (Merged Model)")
print("="*70)

# ========================================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ========================================================================

print(f"ğŸ“¦ Merged ëª¨ë¸ ë¡œë“œ: {MERGED_MODEL_PATH}")
print()

try:
    # â­ Merged ëª¨ë¸ì€ ë°”ë¡œ ë¡œë“œ (PeftModel ë¶ˆí•„ìš”!)
    tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)
    
    print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    print(f"   BOS: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})")
    print(f"   EOS: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")
    print(f"   PAD: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")
    print(f"   Chat template: {tokenizer.chat_template is not None}")
    print()
    
    # â­ Merged ëª¨ë¸ ì§ì ‘ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        MERGED_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    model.eval()
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"   ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
    print()

except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# ========================================================================
# 3. ì¶”ë¡  í•¨ìˆ˜ (Chat Template ì‚¬ìš©)
# ========================================================================

def generate_response(
    question, 
    max_new_tokens=512, 
    temperature=0.7, 
    top_p=0.9,
    repetition_penalty=1.1
):
    """
    Ko-Gemmaì˜ chat_templateì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
    
    chat_template ê·œì¹™:
    - <bos>ë¡œ ì‹œì‘
    - user â†’ user, assistant â†’ modelë¡œ ë³€í™˜
    - add_generation_prompt=Trueë¡œ <start_of_turn>model\n ì¶”ê°€
    """
    
    # â­ messages í¬ë§·ìœ¼ë¡œ ì…ë ¥ êµ¬ì„± (í•„ìˆ˜!)
    messages = [
        {"role": "user", "content": question}
    ]
    
    # â­ chat_template ì ìš© (í•„ìˆ˜!)
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True  # ì¶”ë¡  ì‹œì—ëŠ” True!
    )
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # ë””ì½”ë”© (ì…ë ¥ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ)
    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return answer.strip()

# ========================================================================
# 4. í…ŒìŠ¤íŠ¸
# ========================================================================

print("="*70)
print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œì‘")
print("="*70 + "\n")

test_questions = [
    "How do I get to the College of Human Sciences from Aeji Gate?",
    "í•œì–‘ì—¬ëŒ€ ë³¸ê´€ì—ì„œ í–‰ì›ìŠ¤í€˜ì–´ ì–´ë–»ê²Œ ê°€?",
    "Which building is further away, HIT or the FTC?",
    "507ê´€ì€ ë­ì•¼?",
    "ë³¸ê´€ì€ ë°•ë¬¼ê´€ ì–´ëŠ ìª½ì— ìˆì–´?",
]

for i, question in enumerate(test_questions, 1):
    print(f"\n{'='*70}")
    print(f"Question {i}/{len(test_questions)}")
    print(f"{'='*70}")
    print(f"Q: {question}\n")
    
    # Chat template ì ìš© í™•ì¸
    messages = [{"role": "user", "content": question}]
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    print(f"ğŸ“ ì ìš©ëœ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 200ì):")
    print(repr(prompt[:200]))
    print("-" * 70)
    
    # ë‹µë³€ ìƒì„±
    response = generate_response(question)
    
    print(f"\nA: {response}")
    print(f"{'='*70}\n")

# ========================================================================
# 5. ëŒ€í™”í˜• ëª¨ë“œ
# ========================================================================

print("\n" + "="*70)
print("ğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ")
print("="*70)
print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
print("="*70 + "\n")

while True:
    try:
        user_input = input("You: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not user_input:
            continue
        
        response = generate_response(user_input)
        print(f"\nAI: {response}\n")
        print("-" * 70 + "\n")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    except Exception as e:
        print(f"âš ï¸  ì˜¤ë¥˜ ë°œìƒ: {e}\n")
        import traceback
        traceback.print_exc()

print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
print("="*70)